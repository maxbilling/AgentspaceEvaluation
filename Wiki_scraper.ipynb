{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxbilling/AgentspaceEvaluation/blob/main/Wiki_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UyF_O1ceO3or"
      },
      "id": "UyF_O1ceO3or"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9238390c-97d2-480d-9cc8-50c69e156784",
      "metadata": {
        "tags": [],
        "id": "9238390c-97d2-480d-9cc8-50c69e156784"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 WeasyPrint google-cloud-storage\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user(project_id=project)"
      ],
      "metadata": {
        "id": "u6c5R0jLOiIU"
      },
      "id": "u6c5R0jLOiIU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da60acc1-616d-4e11-9ced-2482f90e34cb",
      "metadata": {
        "tags": [],
        "id": "da60acc1-616d-4e11-9ced-2482f90e34cb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from weasyprint import HTML\n",
        "from google.cloud import storage\n",
        "import uuid\n",
        "import time\n",
        "\n",
        "# --- Configuration ---\n",
        "# GCP Project and GCS Bucket details\n",
        "BUCKET_NAME = \"source_bucket1213092\"  # @param ‚ö†Ô∏è CHANGE THIS to your bucket name\n",
        "\n",
        "# Number of random articles to scrape\n",
        "NUMBER_OF_ARTICLES = 40 # @param\n",
        "\n",
        "# --- End of Configuration ---\n",
        "\n",
        "def get_random_wiki_page():\n",
        "    \"\"\"Fetches the URL of a random English Wikipedia page.\"\"\"\n",
        "    random_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
        "    try:\n",
        "        with requests.get(random_url, timeout=10) as response:\n",
        "            response.raise_for_status()\n",
        "            #print(f\"‚úÖ Fetched random page: {response.url}\")\n",
        "            return response.url\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error fetching random page URL: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_and_clean_content(url):\n",
        "    \"\"\"Scrapes the main content of a Wikipedia page and cleans it for PDF conversion.\"\"\"\n",
        "    try:\n",
        "        with requests.get(url, timeout=10) as response:\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Get the main content div\n",
        "            content_div = soup.find(id=\"mw-content-text\")\n",
        "            if not content_div:\n",
        "                print(\"Could not find main content div.\")\n",
        "                return None, None\n",
        "\n",
        "            # Extract the title of the article\n",
        "            title = soup.find(id=\"firstHeading\").get_text()\n",
        "\n",
        "            # Remove unwanted elements like navigation boxes, edit links, etc.\n",
        "            for element in content_div.find_all(['div', 'span', 'table', 'sup'], class_=['navbox', 'mw-editsection', 'reference', 'reflist']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Construct a clean HTML string for the PDF\n",
        "            # Includes the title and a simple style for better readability\n",
        "            clean_html = f\"\"\"\n",
        "            <html>\n",
        "                <head>\n",
        "                    <meta charset=\"UTF-8\">\n",
        "                    <style>\n",
        "                        body {{ font-family: sans-serif; line-height: 1.6; max-width: 800px; margin: auto; padding: 20px; }}\n",
        "                        h1 {{ color: #333; }}\n",
        "                        img {{ max-width: 100%; height: auto; }}\n",
        "                        a {{ color: #0645ad; text-decoration: none; }}\n",
        "                    </style>\n",
        "                </head>\n",
        "                <body>\n",
        "                    <h1>{title}</h1>\n",
        "                    {str(content_div)}\n",
        "                </body>\n",
        "            </html>\n",
        "            \"\"\"\n",
        "\n",
        "            return clean_html, title\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error scraping content from {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def upload_to_gcs(bucket_name, source_file_object, destination_blob_name):\n",
        "    \"\"\"Uploads an in-memory file object to a GCS bucket.\"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "        # Reset file pointer to the beginning before uploading\n",
        "        source_file_object.seek(0)\n",
        "\n",
        "        blob.upload_from_file(source_file_object, content_type='application/pdf')\n",
        "\n",
        "        #print(f\"üìÑ Successfully uploaded {destination_blob_name} to gs://{bucket_name}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to upload to GCS: {e}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to orchestrate scraping and uploading.\"\"\"\n",
        "    if BUCKET_NAME == \"your-gcs-bucket-name\":\n",
        "        print(\"üö® Please change the BUCKET_NAME variable in the script!\")\n",
        "        return\n",
        "\n",
        "    print(f\"üöÄ Starting scrape of {NUMBER_OF_ARTICLES} articles for bucket '{BUCKET_NAME}'...\")\n",
        "\n",
        "    scraped_count = 0\n",
        "    while scraped_count < NUMBER_OF_ARTICLES:\n",
        "        print(\"-\" * 50)\n",
        "        time.sleep(10)\n",
        "        page_url = get_random_wiki_page()\n",
        "\n",
        "        if not page_url:\n",
        "            continue\n",
        "\n",
        "        html_content, title = scrape_and_clean_content(page_url)\n",
        "\n",
        "        if not html_content or not title:\n",
        "            continue\n",
        "\n",
        "        # Convert sanitized title into a valid filename\n",
        "        safe_filename = \"\".join(c for c in title if c.isalnum() or c in (' ', '_')).rstrip()\n",
        "        pdf_filename = f\"{safe_filename}_{uuid.uuid4().hex[:6]}.pdf\"\n",
        "\n",
        "        try:\n",
        "            # Generate PDF in memory\n",
        "            pdf_bytes = HTML(string=html_content).write_pdf()\n",
        "\n",
        "            # Create an in-memory file-like object\n",
        "            from io import BytesIO\n",
        "            pdf_file_object = BytesIO(pdf_bytes)\n",
        "\n",
        "            # Upload to GCS\n",
        "            if upload_to_gcs(BUCKET_NAME, pdf_file_object, f\"wikipedia-articles/{pdf_filename}\"):\n",
        "                scraped_count += 1\n",
        "                print(f\"Progress: {scraped_count}/{NUMBER_OF_ARTICLES}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå An error occurred during PDF conversion or upload for '{title}': {e}\")\n",
        "\n",
        "    print(f\"\\nüéâ Finished! Scraped and uploaded {scraped_count} articles.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m131",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
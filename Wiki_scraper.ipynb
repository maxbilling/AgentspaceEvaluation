{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxbilling/AgentspaceEvaluation/blob/main/Wiki_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9238390c-97d2-480d-9cc8-50c69e156784",
      "metadata": {
        "tags": [],
        "id": "9238390c-97d2-480d-9cc8-50c69e156784"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 WeasyPrint google-cloud-storage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da60acc1-616d-4e11-9ced-2482f90e34cb",
      "metadata": {
        "tags": [],
        "id": "da60acc1-616d-4e11-9ced-2482f90e34cb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from weasyprint import HTML\n",
        "from google.cloud import storage\n",
        "import uuid\n",
        "import time\n",
        "\n",
        "# --- Configuration ---\n",
        "# GCP Project and GCS Bucket details\n",
        "BUCKET_NAME = \"wikipedia_source_bucket1213092\"  # @param ‚ö†Ô∏è CHANGE THIS to your bucket name\n",
        "\n",
        "# Number of random articles to scrape\n",
        "NUMBER_OF_ARTICLES = 40 # @param\n",
        "\n",
        "# --- End of Configuration ---\n",
        "\n",
        "def get_random_wiki_page():\n",
        "    \"\"\"Fetches the URL of a random English Wikipedia page.\"\"\"\n",
        "    random_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
        "    try:\n",
        "        with requests.get(random_url, timeout=10) as response:\n",
        "            response.raise_for_status()\n",
        "            #print(f\"‚úÖ Fetched random page: {response.url}\")\n",
        "            return response.url\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error fetching random page URL: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_and_clean_content(url):\n",
        "    \"\"\"Scrapes the main content of a Wikipedia page and cleans it for PDF conversion.\"\"\"\n",
        "    try:\n",
        "        with requests.get(url, timeout=10) as response:\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Get the main content div\n",
        "            content_div = soup.find(id=\"mw-content-text\")\n",
        "            if not content_div:\n",
        "                print(\"Could not find main content div.\")\n",
        "                return None, None\n",
        "\n",
        "            # Extract the title of the article\n",
        "            title = soup.find(id=\"firstHeading\").get_text()\n",
        "\n",
        "            # Remove unwanted elements like navigation boxes, edit links, etc.\n",
        "            for element in content_div.find_all(['div', 'span', 'table', 'sup'], class_=['navbox', 'mw-editsection', 'reference', 'reflist']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Construct a clean HTML string for the PDF\n",
        "            # Includes the title and a simple style for better readability\n",
        "            clean_html = f\"\"\"\n",
        "            <html>\n",
        "                <head>\n",
        "                    <meta charset=\"UTF-8\">\n",
        "                    <style>\n",
        "                        body {{ font-family: sans-serif; line-height: 1.6; max-width: 800px; margin: auto; padding: 20px; }}\n",
        "                        h1 {{ color: #333; }}\n",
        "                        img {{ max-width: 100%; height: auto; }}\n",
        "                        a {{ color: #0645ad; text-decoration: none; }}\n",
        "                    </style>\n",
        "                </head>\n",
        "                <body>\n",
        "                    <h1>{title}</h1>\n",
        "                    {str(content_div)}\n",
        "                </body>\n",
        "            </html>\n",
        "            \"\"\"\n",
        "\n",
        "            return clean_html, title\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error scraping content from {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def upload_to_gcs(bucket_name, source_file_object, destination_blob_name):\n",
        "    \"\"\"Uploads an in-memory file object to a GCS bucket.\"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "        # Reset file pointer to the beginning before uploading\n",
        "        source_file_object.seek(0)\n",
        "\n",
        "        blob.upload_from_file(source_file_object, content_type='application/pdf')\n",
        "\n",
        "        #print(f\"üìÑ Successfully uploaded {destination_blob_name} to gs://{bucket_name}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to upload to GCS: {e}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to orchestrate scraping and uploading.\"\"\"\n",
        "    if BUCKET_NAME == \"your-gcs-bucket-name\":\n",
        "        print(\"üö® Please change the BUCKET_NAME variable in the script!\")\n",
        "        return\n",
        "\n",
        "    print(f\"üöÄ Starting scrape of {NUMBER_OF_ARTICLES} articles for bucket '{BUCKET_NAME}'...\")\n",
        "\n",
        "    scraped_count = 0\n",
        "    while scraped_count < NUMBER_OF_ARTICLES:\n",
        "        print(\"-\" * 50)\n",
        "        time.sleep(10)\n",
        "        page_url = get_random_wiki_page()\n",
        "\n",
        "        if not page_url:\n",
        "            continue\n",
        "\n",
        "        html_content, title = scrape_and_clean_content(page_url)\n",
        "\n",
        "        if not html_content or not title:\n",
        "            continue\n",
        "\n",
        "        # Convert sanitized title into a valid filename\n",
        "        safe_filename = \"\".join(c for c in title if c.isalnum() or c in (' ', '_')).rstrip()\n",
        "        pdf_filename = f\"{safe_filename}_{uuid.uuid4().hex[:6]}.pdf\"\n",
        "\n",
        "        try:\n",
        "            # Generate PDF in memory\n",
        "            pdf_bytes = HTML(string=html_content).write_pdf()\n",
        "\n",
        "            # Create an in-memory file-like object\n",
        "            from io import BytesIO\n",
        "            pdf_file_object = BytesIO(pdf_bytes)\n",
        "\n",
        "            # Upload to GCS\n",
        "            if upload_to_gcs(BUCKET_NAME, pdf_file_object, f\"wikipedia-articles/{pdf_filename}\"):\n",
        "                scraped_count += 1\n",
        "                print(f\"Progress: {scraped_count}/{NUMBER_OF_ARTICLES}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå An error occurred during PDF conversion or upload for '{title}': {e}\")\n",
        "\n",
        "    print(f\"\\nüéâ Finished! Scraped and uploaded {scraped_count} articles.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd474de-32d2-4d8e-b4cf-0171c0d958b3",
      "metadata": {
        "tags": [],
        "id": "acd474de-32d2-4d8e-b4cf-0171c0d958b3",
        "outputId": "c36f2d3a-ac82-4115-fe9b-6b3569a51739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (2.19.0)\n",
            "Requirement already satisfied: google-generativeai in /opt/conda/lib/python3.10/site-packages (0.8.5)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.40.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.32.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2025.7.14)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-python-client in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (2.176.0)\n",
            "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->google-generativeai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-storage google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9213d286-f960-4306-b920-39b0f64c8417",
      "metadata": {
        "tags": [],
        "id": "9213d286-f960-4306-b920-39b0f64c8417",
        "outputId": "2d4d05ef-f294-4eef-8a1a-0662bd527b19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jupyter/.local/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Searching for PDF files in gs://wikipedia_source_bucket1213092/wikipedia-articles/...\n",
            "‚úÖ Found 96 PDF files.\n",
            "--------------------------------------------------\n",
            "Processing file 1 of 96\n",
            "üß† Processing 'wikipedia-articles/2002 US Open  Womens singles qualifying_a96b60.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 2 of 96\n",
            "üß† Processing 'wikipedia-articles/2022 European Speed Skating Championships  Womens team sprint_abd5c1.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 3 of 96\n",
            "üß† Processing 'wikipedia-articles/40th Army Soviet Union_4b5e96.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 4 of 96\n",
            "üß† Processing 'wikipedia-articles/A Fighting Chance memoir_18155c.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 5 of 96\n",
            "üß† Processing 'wikipedia-articles/AFC Euro Kickers_01ce62.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 6 of 96\n",
            "üß† Processing 'wikipedia-articles/Abang Long Fadil_396854.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 7 of 96\n",
            "üß† Processing 'wikipedia-articles/Amelia Watson_990576.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 8 of 96\n",
            "üß† Processing 'wikipedia-articles/Athletics at the 1999 Summer Universiade_60f773.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 9 of 96\n",
            "üß† Processing 'wikipedia-articles/Auli India_c1858f.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 10 of 96\n",
            "üß† Processing 'wikipedia-articles/Backdrop Peak_7fe501.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 11 of 96\n",
            "üß† Processing 'wikipedia-articles/Badacsonytomaj_a0f2c2.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 12 of 96\n",
            "üß† Processing 'wikipedia-articles/Berhan Bank_3778dd.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 13 of 96\n",
            "üß† Processing 'wikipedia-articles/Boothipuram_08d877.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 14 of 96\n",
            "üß† Processing 'wikipedia-articles/Charles Chamois_5fd37c.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 15 of 96\n",
            "üß† Processing 'wikipedia-articles/Charles Provis_f8172c.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 16 of 96\n",
            "üß† Processing 'wikipedia-articles/Clarksville Florida_06036c.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 17 of 96\n",
            "üß† Processing 'wikipedia-articles/Claude Bremond_103829.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 18 of 96\n",
            "üß† Processing 'wikipedia-articles/CochranHeltonLindley House_d121d6.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 19 of 96\n",
            "üß† Processing 'wikipedia-articles/Cosmopterix chaldene_84aeca.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 20 of 96\n",
            "üß† Processing 'wikipedia-articles/Creve Coeur Lake Memorial Park_edc364.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 21 of 96\n",
            "üß† Processing 'wikipedia-articles/Dan Goggin composer_513519.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 22 of 96\n",
            "üß† Processing 'wikipedia-articles/Dashte Qazi_c0b566.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 23 of 96\n",
            "üß† Processing 'wikipedia-articles/Dont Play the Fool_9d537c.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 24 of 96\n",
            "üß† Processing 'wikipedia-articles/Electoral division of Murchison_edf07a.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 25 of 96\n",
            "üß† Processing 'wikipedia-articles/Elizabeth F Fisher_c0c208.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 26 of 96\n",
            "üß† Processing 'wikipedia-articles/Erica Moore politician_4bafa5.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 27 of 96\n",
            "üß† Processing 'wikipedia-articles/Estadio Ol√≠mpico Pascual Guerrero_5cb812.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 28 of 96\n",
            "üß† Processing 'wikipedia-articles/Evander_2d8694.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 29 of 96\n",
            "üß† Processing 'wikipedia-articles/Event segment_31dbb6.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 30 of 96\n",
            "üß† Processing 'wikipedia-articles/Ezequiel Gonz√°lez Mas_75280f.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 31 of 96\n",
            "üß† Processing 'wikipedia-articles/Fountain Green Illinois_257ebc.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 32 of 96\n",
            "üß† Processing 'wikipedia-articles/FrieslandCampina_2b8634.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 33 of 96\n",
            "üß† Processing 'wikipedia-articles/German submarine U127_17bf01.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 34 of 96\n",
            "üß† Processing 'wikipedia-articles/Glenn Thomaris_cfbc09.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 35 of 96\n",
            "üß† Processing 'wikipedia-articles/Government of Lithuania_4ba7d2.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 36 of 96\n",
            "üß† Processing 'wikipedia-articles/Hadogenes gunningi_678521.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 37 of 96\n",
            "üß† Processing 'wikipedia-articles/Hanƒ±mzer Melet_27f445.pdf' with Vertex AI Gemini...\n",
            "‚ö†Ô∏è Warning: Could not parse Q&A from response for wikipedia-articles/Hanƒ±mzer Melet_27f445.pdf.\n",
            "--------------------------------------------------\n",
            "Processing file 38 of 96\n",
            "üß† Processing 'wikipedia-articles/Herpetogramma yaeyamense_9c2278.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 39 of 96\n",
            "üß† Processing 'wikipedia-articles/Hull derby_6f1483.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 40 of 96\n",
            "üß† Processing 'wikipedia-articles/Iced_f572f5.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 41 of 96\n",
            "üß† Processing 'wikipedia-articles/Inguromorpha arcifera_016781.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 42 of 96\n",
            "üß† Processing 'wikipedia-articles/Iod river_b39c4b.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 43 of 96\n",
            "üß† Processing 'wikipedia-articles/John Bradley Australian politician_00d628.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 44 of 96\n",
            "üß† Processing 'wikipedia-articles/John G Griffith_172ab6.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 45 of 96\n",
            "üß† Processing 'wikipedia-articles/Julie Ann Dawson_62d3c6.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 46 of 96\n",
            "üß† Processing 'wikipedia-articles/Kellermann_5bc542.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 47 of 96\n",
            "üß† Processing 'wikipedia-articles/Kellerville Indiana_d2143d.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 48 of 96\n",
            "üß† Processing 'wikipedia-articles/Laurence Woodward Jessup_1777e8.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 49 of 96\n",
            "üß† Processing 'wikipedia-articles/List of Welsh statutory instruments 2011_cd4ea4.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 50 of 96\n",
            "üß† Processing 'wikipedia-articles/List of islands of Arizona_4aac18.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 51 of 96\n",
            "üß† Processing 'wikipedia-articles/Live at Tramps NYC 1996_fcb0fa.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 52 of 96\n",
            "üß† Processing 'wikipedia-articles/Living room_ac91c8.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 53 of 96\n",
            "üß† Processing 'wikipedia-articles/Luke Fagan_b0fe2a.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 54 of 96\n",
            "üß† Processing 'wikipedia-articles/Magherafelt Area C_8d970a.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 55 of 96\n",
            "üß† Processing 'wikipedia-articles/Manjari Makijany_9df41a.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 56 of 96\n",
            "üß† Processing 'wikipedia-articles/Marvin De Lima_8d5efc.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 57 of 96\n",
            "üß† Processing 'wikipedia-articles/Mattair Springs_864695.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 58 of 96\n",
            "üß† Processing 'wikipedia-articles/Mercury and Two Allegorical Figures_8f6bbf.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 59 of 96\n",
            "üß† Processing 'wikipedia-articles/Moxley West Midlands_2561ae.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 60 of 96\n",
            "üß† Processing 'wikipedia-articles/Nathaniel Rothschild 5th Baron Rothschild_d6f12b.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 61 of 96\n",
            "üß† Processing 'wikipedia-articles/Nicole Forrester_43b9bd.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 62 of 96\n",
            "üß† Processing 'wikipedia-articles/Orla OReilly_bf1d3a.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 63 of 96\n",
            "üß† Processing 'wikipedia-articles/Oziotelphusa minneriyaensis_f8a174.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 64 of 96\n",
            "üß† Processing 'wikipedia-articles/Paddle_9d3039.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 65 of 96\n",
            "üß† Processing 'wikipedia-articles/Pardee Dam_ee800a.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 66 of 96\n",
            "üß† Processing 'wikipedia-articles/Performance Ranking of Scientific Papers for World Universities_bf1a32.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 67 of 96\n",
            "üß† Processing 'wikipedia-articles/Pine River Quebec_7ebc2f.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 68 of 96\n",
            "üß† Processing 'wikipedia-articles/Port of P√§rnu_c2a368.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 69 of 96\n",
            "üß† Processing 'wikipedia-articles/Rayna given name_3efab7.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 70 of 96\n",
            "üß† Processing 'wikipedia-articles/Rebecca McKenna_0f61d5.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 71 of 96\n",
            "üß† Processing 'wikipedia-articles/Relva_f95557.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 72 of 96\n",
            "üß† Processing 'wikipedia-articles/Roe_648393.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 73 of 96\n",
            "üß† Processing 'wikipedia-articles/Roj Shaweis_01bb78.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 74 of 96\n",
            "üß† Processing 'wikipedia-articles/Russell House Bedford Pennsylvania_9853a0.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 75 of 96\n",
            "üß† Processing 'wikipedia-articles/Sacred trees and groves in Germanic paganism and mythology_4a913e.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 76 of 96\n",
            "üß† Processing 'wikipedia-articles/Sara Headley_80c0fc.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 77 of 96\n",
            "üß† Processing 'wikipedia-articles/Scott Laycock_77e9a9.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 78 of 96\n",
            "üß† Processing 'wikipedia-articles/Second Council of Toledo_57215d.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 79 of 96\n",
            "üß† Processing 'wikipedia-articles/Solo Gonzalo Rubalcaba album_ef2993.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 80 of 96\n",
            "üß† Processing 'wikipedia-articles/Spain at the UCI Track Cycling World Championships_d2951f.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 81 of 96\n",
            "üß† Processing 'wikipedia-articles/Subhrajit Dutta_43e140.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 82 of 96\n",
            "üß† Processing 'wikipedia-articles/Suillellus amygdalinus_b34dad.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 83 of 96\n",
            "üß† Processing 'wikipedia-articles/Sujit Wongthes_920f3f.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 84 of 96\n",
            "üß† Processing 'wikipedia-articles/Svetlyakclass patrol boat_dc02f3.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 85 of 96\n",
            "üß† Processing 'wikipedia-articles/Teresa P√†mies_72c248.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 86 of 96\n",
            "üß† Processing 'wikipedia-articles/The Coming of Atlas_12d095.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 87 of 96\n",
            "üß† Processing 'wikipedia-articles/The Dissertation novel_629489.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 88 of 96\n",
            "üß† Processing 'wikipedia-articles/They Went ThatAWay  ThatAWay_8c8933.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 89 of 96\n",
            "üß† Processing 'wikipedia-articles/This Is the Remix Destinys Child album_0f8b49.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 90 of 96\n",
            "üß† Processing 'wikipedia-articles/Trechus dilutus_442a79.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 91 of 96\n",
            "üß† Processing 'wikipedia-articles/V2 song_fae5c0.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 92 of 96\n",
            "üß† Processing 'wikipedia-articles/Vulcan V18_a7cd31.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 93 of 96\n",
            "üß† Processing 'wikipedia-articles/Weightlifting at the Summer World University Games_f292ad.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 94 of 96\n",
            "üß† Processing 'wikipedia-articles/World Tourism Day_ac2eed.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 95 of 96\n",
            "üß† Processing 'wikipedia-articles/Zarubin_5cbe51.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "--------------------------------------------------\n",
            "Processing file 96 of 96\n",
            "üß† Processing 'wikipedia-articles/Zavala County Courthouse_650df4.pdf' with Vertex AI Gemini...\n",
            "üëç Successfully generated 2 Q&A pairs.\n",
            "\n",
            "‚úÖ Successfully uploaded results to gs://wikipedia_source_bucket1213092/wikipedia-articles/input_queries.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import csv\n",
        "import datetime\n",
        "import google.auth\n",
        "\n",
        "# Import the necessary Google Cloud libraries\n",
        "from google.cloud import storage\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "# ‚ö†Ô∏è CHANGE these to match your GCP environment and bucket\n",
        "GCP_PROJECT_ID = \"maxbproject\"\n",
        "GCP_REGION = \"europe-west1\" # Or your specific region\n",
        "BUCKET_NAME = \"wikipedia_source_bucket1213092\"\n",
        "\n",
        "# The folder where the PDFs are stored\n",
        "PDF_FOLDER = \"wikipedia-articles/\"\n",
        "\n",
        "# The name for the output CSV file\n",
        "CSV_OUTPUT_FILENAME = \"input_queries.csv\"\n",
        "\n",
        "# ‚ö†Ô∏è PASTE THE SERVICE ACCOUNT EMAIL you found in Step 1 here\n",
        "SERVICE_ACCOUNT_EMAIL = \"694598770214-compute@developer.gserviceaccount.com\"\n",
        "\n",
        "\n",
        "GEMINI_PROMPT = \"\"\"\n",
        "Based on the Wikipedia article attached, generate two questions that can be answered with the information in the PDF.\n",
        "The questions should not be generic but detailed enough that they can be answered only with the information in the PDF.\n",
        "Respond ONLY in the following format, with each Q&A pair on a new line:\n",
        "search_query;expected_answer\n",
        "Ensure that question and answer are always filled. If you cannot generate a question and answer pair reponde with \"could not genreate question\"; \"could not generate answer\".\n",
        "\"\"\"\n",
        "# --- End of Configuration ---\n",
        "\n",
        "def list_pdf_blobs(storage_client, bucket_name, folder_name):\n",
        "    print(f\"üîç Searching for PDF files in gs://{bucket_name}/{folder_name}...\")\n",
        "    blobs = storage_client.list_blobs(bucket_name, prefix=folder_name)\n",
        "    pdf_blobs = [blob for blob in blobs if blob.name.lower().endswith(\".pdf\")]\n",
        "    print(f\"‚úÖ Found {len(pdf_blobs)} PDF files.\")\n",
        "    return pdf_blobs\n",
        "\n",
        "def generate_qna_from_pdf(model, pdf_blob):\n",
        "    print(f\"üß† Processing '{pdf_blob.name}' with Vertex AI Gemini...\")\n",
        "    try:\n",
        "        pdf_content = pdf_blob.download_as_bytes()\n",
        "        pdf_file_for_api = Part.from_data(data=pdf_content, mime_type=\"application/pdf\")\n",
        "        request_payload = [GEMINI_PROMPT, pdf_file_for_api]\n",
        "        response = model.generate_content(request_payload)\n",
        "        qna_pairs = []\n",
        "        for line in response.text.strip().split('\\n'):\n",
        "            if ';' in line:\n",
        "                parts = line.split(';', 1)\n",
        "                if len(parts) == 2:\n",
        "                    qna_pairs.append({\"search_query\": parts[0].strip(), \"expected_answer\": parts[1].strip()})\n",
        "        if not qna_pairs:\n",
        "            print(f\"‚ö†Ô∏è Warning: Could not parse Q&A from response for {pdf_blob.name}.\")\n",
        "        else:\n",
        "            print(f\"üëç Successfully generated {len(qna_pairs)} Q&A pairs.\")\n",
        "        return qna_pairs\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred while processing {pdf_blob.name}: {e}\")\n",
        "        return []\n",
        "\n",
        "# MODIFIED FUNCTION\n",
        "import urllib.parse\n",
        "\n",
        "def generate_mtls_url(blob):\n",
        "    \"\"\"\n",
        "    Generates a permanent, authenticated mTLS URL for a GCS blob.\n",
        "\n",
        "    This URL requires the user to authenticate via IAM and mTLS. Spaces in the\n",
        "    object name are correctly URL-encoded.\n",
        "    \"\"\"\n",
        "    # Get the bucket and object names from the blob object.\n",
        "    bucket_name = blob.bucket.name\n",
        "    object_name = blob.name\n",
        "\n",
        "    # URL-encode the object name to handle spaces (e.g., ' ' -> '%20')\n",
        "    # and other special characters, while keeping '/' for folder paths.\n",
        "    encoded_object_name = urllib.parse.quote(object_name, safe='/')\n",
        "\n",
        "    # Construct the URL using the specified format.\n",
        "    mtls_url = f\"gs://{bucket_name}/{object_name}\"\n",
        "\n",
        "    return mtls_url\n",
        "\n",
        "def upload_csv_to_gcs(storage_client, bucket_name, destination_path, data):\n",
        "    if not data:\n",
        "        print(\"No data to upload. Skipping CSV creation.\")\n",
        "        return\n",
        "    string_io = io.StringIO()\n",
        "    fieldnames = ['search_query', 'expected_answer', 'golden_url']\n",
        "    writer = csv.DictWriter(string_io, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(data)\n",
        "    csv_data = string_io.getvalue()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_path)\n",
        "    blob.upload_from_string(csv_data, content_type='text/csv')\n",
        "    print(f\"\\n‚úÖ Successfully uploaded results to gs://{bucket_name}/{destination_path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    if \"your-gcp-project-id\" in GCP_PROJECT_ID or \"your-notebook-service-account\" in SERVICE_ACCOUNT_EMAIL:\n",
        "        print(\"üö® Please update the GCP_PROJECT_ID and SERVICE_ACCOUNT_EMAIL variables in the Configuration section!\")\n",
        "        return\n",
        "\n",
        "    credentials, project = google.auth.default()\n",
        "\n",
        "    try:\n",
        "        vertexai.init(project=GCP_PROJECT_ID, location=GCP_REGION)\n",
        "        model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "        storage_client = storage.Client()\n",
        "\n",
        "        pdf_blobs = list_pdf_blobs(storage_client, BUCKET_NAME, PDF_FOLDER)\n",
        "        if not pdf_blobs:\n",
        "            return\n",
        "\n",
        "        all_results = []\n",
        "        for i, blob in enumerate(pdf_blobs):\n",
        "            print(\"-\" * 50)\n",
        "            print(f\"Processing file {i+1} of {len(pdf_blobs)}\")\n",
        "            qna_list = generate_qna_from_pdf(model, blob)\n",
        "            if qna_list:\n",
        "                # MODIFIED FUNCTION CALL\n",
        "                golden_url = generate_mtls_url(blob)\n",
        "                for item in qna_list:\n",
        "                    all_results.append({**item, \"golden_url\": golden_url})\n",
        "\n",
        "        csv_destination_path = f\"{PDF_FOLDER.strip('/')}/{CSV_OUTPUT_FILENAME}\"\n",
        "        upload_csv_to_gcs(storage_client, BUCKET_NAME, csv_destination_path, all_results)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred during initialization or execution: {e}\")\n",
        "\n",
        "# --- Run the main function ---\n",
        "main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff4473e-69c6-4c72-b92a-888bbaa11ef2",
      "metadata": {
        "id": "0ff4473e-69c6-4c72-b92a-888bbaa11ef2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m131",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
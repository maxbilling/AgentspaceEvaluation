{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxbilling/AgentspaceEvaluation/blob/main/%5BTEMPLATE%5D_AgentspaceConnectorValidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9769c2eb-6dcf-40b2-971d-8138b565ec0a",
      "metadata": {
        "id": "9769c2eb-6dcf-40b2-971d-8138b565ec0a"
      },
      "source": [
        "Author: @maxbilling\n",
        "Based on: go/agentspace-eval-customer\n",
        "\n",
        "Run Search Quality and Generated Answer Quality Evaluations\n",
        "This notebook helps you to automate running sevaral search queries. The generated reponse to the question and top K search results are saved to a file for further review and human assessment.\n",
        "Steps:\n",
        "\n",
        "Create a csv file with columns:\n",
        "\n",
        "*   'search_query'\n",
        "*   'expected_answer'\n",
        "*   'golden_url'\n",
        "\n",
        "\n",
        "***'golden_url'***: Add a link to the golden response if you have one. You can also provide multiple links separated by a new line. This is for future evaluation to autogenerate metrics\n",
        "\n",
        "Durin evaluation the following steps are performed:\n",
        "5.0 search queries are imported from the file\n",
        "6.1 a semantic search is performed with the default_search:search configuration of your agentspace app\n",
        "6.2 based on the serach session a answer is genreated with the default_search:aswer configuraiton of your agentspace app\n",
        "6.3 both results are combined the results are written back to the file\n",
        "\n",
        "After the search and aswer results are written a evaluation takes place with\n",
        "\n",
        "8.2. search and answer results are loaded from file\n",
        "8.3. one by one evaluated by two models defined in 8.1. One model to evaluate the quality of the answer the second model to evaluate if the golden_url was part of the urls found.\n",
        "8.4 the results are saved to file\n",
        "\n",
        "\n",
        "**Prerequisists**\n",
        "\n",
        "\n",
        "*   Input csv with search_query, expected_answer, golden_url\n",
        "*   GCS bucket for temporary file storage\n",
        "*   BQ Dataset to store Evaluation output\n",
        "\n",
        "\n",
        "**Permissions**\n",
        "\n",
        "*   Agentspace: roles/discoveryengine.user\n",
        "*   GCS: roles/storage.objectAdmin\n",
        "*   BQ: roles/bigquery.dataEditor\n",
        "\n",
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E16fqffhZe_x",
      "metadata": {
        "id": "E16fqffhZe_x"
      },
      "source": [
        "# 1.0 Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b095a2-2ad7-4531-add1-a83213718223",
      "metadata": {
        "id": "83b095a2-2ad7-4531-add1-a83213718223"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet colabtools\n",
        "!pip install --quiet requests-toolbelt==1.0.0\n",
        "!pip install --quiet google-cloud-discoveryengine\n",
        "!pip install --upgrade --user --quiet google-cloud-aiplatform\n",
        "!pip install --upgrade --quiet  langchain-core langchain-google-vertexai\n",
        "!pip install --upgrade --quiet json5\n",
        "!pip install --quiet scipy\n",
        "!pip install --quiet gspread\n",
        "!pip install --quiet --upgrade pandas pyarrow pandas-gbq\n",
        "!pip install --quiet upgrade google-generativeai\n",
        "!pip install --quiet tenacity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OF0gvLdCZo9x",
      "metadata": {
        "id": "OF0gvLdCZo9x"
      },
      "source": [
        "# 2.0 Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "21f4499c-e6d6-4cd3-b3e6-9ed571a6f633",
      "metadata": {
        "id": "21f4499c-e6d6-4cd3-b3e6-9ed571a6f633"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import logging\n",
        "import datetime\n",
        "import requests\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "\n",
        "from google.auth import default\n",
        "from google.colab import auth\n",
        "import google.auth.transport.requests\n",
        "import vertexai\n",
        "import gspread\n",
        "\n",
        "# Logger Setup\n",
        "logger = logging.getLogger()\n",
        "logging.basicConfig(filename='spark_eval_notebook_logs.log', filemode='a', level=logging.DEBUG)\n",
        "\n",
        "creds, _ = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DE4CFo_WZ6N8",
      "metadata": {
        "id": "DE4CFo_WZ6N8"
      },
      "source": [
        "# 3.0 Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "985f0f6e-6a2b-455d-98d9-dc20e1cd94ea",
      "metadata": {
        "id": "985f0f6e-6a2b-455d-98d9-dc20e1cd94ea"
      },
      "outputs": [],
      "source": [
        "# Use project number and not Project ID\n",
        "connector= \"\" # @param {\"type\":\"string\",\"placeholder\":\"e.g. 1234567890\"}\n",
        "project_num = \"\" # @param {\"type\":\"string\",\"placeholder\":\"e.g. 1234567890\"}\n",
        "project = \"\" # @param {\"type\":\"string\",\"placeholder\":\"e.g. 1234567890\"}\n",
        "# Engine ID is the ID of the Agent builder APP\n",
        "engine_id = \"\" # @param {\"type\":\"string\",\"placeholder\":\"e.g. my-search-app_12345\"}\n",
        "\n",
        "# Assist or Search - Note: \"Assist\" means \"Search + Assist\"\n",
        "# Whereas Search means \"Search + Answer\"\n",
        "app_type = \"assist\" # @param [\"search\",\"assist\"]\n",
        "\n",
        "# Location\n",
        "location = 'eu' # @param {\"type\":\"string\",\"placeholder\":\"Set: \\\"global\\\" \"}\n",
        "\n",
        "# evaluation region, global is not supported yet\n",
        "evaluation_region = \"eu\" # @param {\"type\":\"string\",\"placeholder\":\"evaluation region, global is not supported yet\"}\n",
        "vertex_region = 'europe-west1' # @param {\"type\":\"string\",\"placeholder\":\"Set: \\\"global\\\" \"}\n",
        "# Use Project ID\n",
        "auth_project_id = '' # @param {\"type\":\"string\",\"placeholder\":\"e.g. my-project-id\"}\n",
        "\n",
        "# Input Queries\n",
        "# Note: Every user needs to have their own copy of this doc. Please make a copy of the golden data google sheet below and add that link.\n",
        "# two columns named search_query, expected_answer\n",
        "bucket = \"\" # @param {\"type\":\"string\",\"placeholder\":\"Google Sheets file URL\"}\n",
        "folder = \"\" # @param {\"type\":\"string\",\"placeholder\":\"Google Sheets file URL\"}\n",
        "output_file_name = 'eval_output' # @param {\"type\":\"string\",\"placeholder\":\"e.g. test_output1\"}\n",
        "\n",
        "output_dataset= 'agentspace_validation' # @param {\"type\":\"string\",\"placeholder\":\"e.g. dataset\"}\n",
        "# Input Queries - Note: Queries tab name within Google Sheets file\n",
        "input_file_name = 'input_queries' # @param {\"type\":\"string\",\"placeholder\":\"e.g. 'input_queries'\"}\n",
        "\n",
        "# (Optional) Output file name used for debugging. This file will be saved in the colab env.\n",
        "\n",
        "# Eval data worksheet\n",
        "sheet_name_suffix = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "do_evaluation = True # @param {\"type\":\"boolean\",\"placeholder\":\"run evaluation\"}\n",
        "\n",
        "\n",
        "WORKFORCE_POOL_ID=\"\" # @param {\"type\":\"string\", \"placeholder\":\"pool id (only if using wif)\"}\n",
        "WORKFORCE_PROVIDER_ID=\"\" # @param {\"type\":\"string\", \"placeholder\":\"provider id (only if using wif)\"}\n",
        "WIF_CONFIG_FILE=\"my_wif_auth_config.json\"\n",
        "\n",
        "# session user pseudo id\n",
        "session_user_pseudo_id = \"testuser01\" # @param {\"type\":\"string\",\"placeholder\":\"session user pseudo id\"}\n",
        "\n",
        "# Top 'K' for search Results\n",
        "K = \"10\" # @param {\"type\":\"string\"}\n",
        "\n",
        "serving_config_id = \"default_search\" # @param {\"type\":\"string\",\"placeholder\":\"default_search\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sk1PEy89aLxR",
      "metadata": {
        "id": "Sk1PEy89aLxR"
      },
      "source": [
        "# 4.0 Authenticate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Vzrf4EiaSK_",
      "metadata": {
        "id": "3Vzrf4EiaSK_"
      },
      "source": [
        "\n",
        "## 4.1 glcoud - Google User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9a9845c9-27c1-46f7-b9c1-0bffd539b70e",
      "metadata": {
        "id": "9a9845c9-27c1-46f7-b9c1-0bffd539b70e"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user(project_id=project)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2YFvxEkOruwT",
      "metadata": {
        "id": "2YFvxEkOruwT"
      },
      "source": [
        "## 4.2 glcoud - Workforce Identity Federated User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PTcX2K5Lrtps",
      "metadata": {
        "id": "PTcX2K5Lrtps"
      },
      "outputs": [],
      "source": [
        "!gcloud iam workforce-pools create-login-config \\\n",
        "    locations/global/workforcePools/{WORKFORCE_POOL_ID}/providers/{WORKFORCE_PROVIDER_ID} \\\n",
        "    --output-file={WIF_CONFIG_FILE}\n",
        "!gcloud auth login --no-launch-browser --login-config={WIF_CONFIG_FILE}\n",
        "!gcloud auth application-default set-quota-project {project}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ImQZc0vmtlX",
      "metadata": {
        "id": "4ImQZc0vmtlX"
      },
      "source": [
        "## 4.1 Obtain Token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5ca97b-0830-426e-8a08-14d7c5a44690",
      "metadata": {
        "id": "4e5ca97b-0830-426e-8a08-14d7c5a44690"
      },
      "outputs": [],
      "source": [
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "\n",
        "def gcloud_access_token_wif():\n",
        "  ato = !gcloud auth print-access-token\n",
        "  ato = list(ato)[0].strip()\n",
        "  return ato\n",
        "\n",
        "def get_access_token():\n",
        "  \"\"\"\n",
        "  Finds default credentials and returns a valid access token.\n",
        "  Works for both standard users and Workforce Identity users who have already logged in.\n",
        "  \"\"\"\n",
        "  if WORKFORCE_POOL_ID and WORKFORCE_PROVIDER_ID:\n",
        "    if not access_token:\n",
        "      access_token = gcloud_access_token_wif()\n",
        "    return access_token\n",
        "\n",
        "  # Find the application default credentials configured for the environment.\n",
        "  creds, project_id = google.auth.default()\n",
        "  # Create a transport request object to refresh the credentials.\n",
        "  request = google.auth.transport.requests.Request()\n",
        "  # Refresh the credentials to ensure the token is valid and not expired.\n",
        "  creds.refresh(request)\n",
        "  return creds.token\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Get and print the token ---\n",
        "access_token = get_access_token()\n",
        "print(f\"\\nAccess Token: {access_token[:20]}...\") # Print first 20 chars for security\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kQ2knsnpahLp",
      "metadata": {
        "id": "kQ2knsnpahLp"
      },
      "source": [
        "# 5.0 Import Search Queries, Expected Answers and Golden URLs from File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30170a2e-d956-4d93-bd63-82c8b30e2b67",
      "metadata": {
        "id": "30170a2e-d956-4d93-bd63-82c8b30e2b67"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "file_name = \"confluence_qa_pairs.csv\"\n",
        "# 1. Define the GCS path to your CSV file.\n",
        "gcs_uri = f\"gs://{bucket}/{folder}/{input_file_name}.csv\"\n",
        "print(f\"GCS path: {gcs_uri}\")\n",
        "\n",
        "# 2. IMPORTANT: Define the original column names from your CSV file that you want to read.\n",
        "#    Replace these placeholder names with the actual names in your file.\n",
        "original_cols_to_read = ['search_query', 'expected_answer', 'golden_url']\n",
        "\n",
        "# Define the new names for your DataFrame columns.\n",
        "new_column_names = {\n",
        "    'search_query': 'search_query',\n",
        "    'expected_answer': 'expected_answer',\n",
        "    'golden_url': 'golden_url'\n",
        "}\n",
        "\n",
        "# 3. Read only the specified columns from GCS and rename them in one step.\n",
        "print(f\"Reading data from GCS bucket: {gcs_uri}\")\n",
        "golden_data = pd.read_csv(\n",
        "    gcs_uri,\n",
        "    usecols=original_cols_to_read  # Only load these specific columns\n",
        ").rename(columns=new_column_names) # Rename them to your desired names\n",
        "\n",
        "# 4. Display the first 10 rows and columns to confirm it loaded correctly.\n",
        "print(\"\\n--- DataFrame Info ---\")\n",
        "golden_data.info()\n",
        "\n",
        "print(\"\\n--- DataFrame Head ---\")\n",
        "print(golden_data.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xnGJPrkPa0CD",
      "metadata": {
        "id": "xnGJPrkPa0CD"
      },
      "source": [
        "# 6.0 Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iwJ0z-GkbRyn",
      "metadata": {
        "id": "iwJ0z-GkbRyn"
      },
      "source": [
        "## 6.1 Get Search Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "304142f6-0d36-465a-8f44-40edf3a3fb88",
      "metadata": {
        "id": "304142f6-0d36-465a-8f44-40edf3a3fb88"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "# Assume logger and other variables (project_num, K, etc.) are defined elsewhere\n",
        "\n",
        "def get_search_results(query, num=K, max_attempts=5):\n",
        "    \"\"\"\n",
        "    Searches for a query and retries up to 'max_attempts' times if no results are found.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_attempts):\n",
        "\n",
        "        try:\n",
        "            access_token = get_access_token()\n",
        "            url = f\"https://{evaluation_region}-discoveryengine.googleapis.com/v1alpha/projects/{project_num}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/{serving_config_id}:search\"\n",
        "\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {access_token}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "            data = {\n",
        "                # \"servingConfig\": f\"projects/{project_num}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/{serving_config_id}\",\n",
        "                \"query\": query,\n",
        "                \"pageSize\": num,\n",
        "                \"relevanceScoreSpec\":{\"returnRelevanceScore\":True},\n",
        "                \"contentSearchSpec\":{\"snippetSpec\":{\"returnSnippet\":True}},\n",
        "                \"session\":f\"projects/{project_num}/locations/{location}/collections/default_collection/engines/{engine_id}/sessions/-\"\n",
        "            }\n",
        "\n",
        "            search_response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "            if search_response.status_code == 200:\n",
        "                response_data = search_response.json()\n",
        "                results = response_data.get('results')\n",
        "                #print(json.dumps(response_data, indent=2))\n",
        "                # If results are found, process and return immediately.\n",
        "                if results:\n",
        "                    logger.info(f\"Query '{query}' found results on attempt {attempt + 1}.\")\n",
        "                    return search_response.json(), attempt+1\n",
        "\n",
        "                # If no results, the loop will continue to the next attempt.\n",
        "                #logger.warning(f\"Query '{query}' SEARCH QUERY returned no results on attempt {attempt + 1}/{max_attempts}\")\n",
        "\n",
        "            else:\n",
        "                # Log API errors but still continue to retry for transient server issues\n",
        "                logger.error(\n",
        "                    f\"API call for query '{query}' failed on attempt {attempt + 1} with status \"\n",
        "                    f\"{search_response.status_code}: {search_response.text}\"\n",
        "                )\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"A network error occurred on attempt {attempt + 1} for query '{query}': {e}\")\n",
        "\n",
        "        # If this is not the last attempt, wait before retrying.\n",
        "        if attempt < max_attempts - 1:\n",
        "            backoff = (attempt+1) * 2 * (attempt+1)\n",
        "            time.sleep(backoff) # exponentailly back of retry\n",
        "\n",
        "    # This line is reached only if all attempts failed to find results.\n",
        "    logger.error(f\"Query '{query}' failed to retrieve results after {max_attempts} attempts.\")\n",
        "    return None,0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jTi7Du6ya-vn",
      "metadata": {
        "id": "jTi7Du6ya-vn"
      },
      "source": [
        "## 6.2 Get Answer Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3045f562-22a1-4a4c-840c-0fe9206a99ef",
      "metadata": {
        "id": "3045f562-22a1-4a4c-840c-0fe9206a99ef"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "# Assume logger and other variables (creds_token, evaluation_region, etc.) are defined\n",
        "\n",
        "def get_answer_results(query, queryId, session_name, max_attempts=2):\n",
        "    for attempt in range(max_attempts):\n",
        "\n",
        "        try:\n",
        "            access_token = get_access_token()\n",
        "\n",
        "            url = f\"https://{evaluation_region}-discoveryengine.googleapis.com/v1alpha/projects/{project_num}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/{serving_config_id}:answer\"\n",
        "\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {access_token}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "\n",
        "            # The data dictionary with corrected Python syntax\n",
        "            data = {\n",
        "                \"servingConfig\": f\"projects/{project_num}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/{serving_config_id}\",\n",
        "                \"query\": {\"text\": query, \"queryId\": queryId},\n",
        "                \"session\": session_name,\n",
        "                \"relatedQuestionsSpec\": {\"enable\": True},\n",
        "                \"answerGenerationSpec\": {\n",
        "                    \"ignoreAdversarialQuery\": True,\n",
        "                    \"ignoreNonAnswerSeekingQuery\": False,\n",
        "                    \"ignoreLowRelevantContent\": True,\n",
        "                    \"multimodalSpec\": {},\n",
        "                    \"includeCitations\": True\n",
        "                }\n",
        "            }\n",
        "\n",
        "            search_response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "            if search_response.status_code == 200:\n",
        "                response_data = search_response.json()\n",
        "                #print(json.dumps(response_data, indent=2))\n",
        "                answer = response_data.get('answer')\n",
        "                if answer:\n",
        "                  try:\n",
        "                      # Returns the full JSON response if successful\n",
        "\n",
        "                      return search_response.json()\n",
        "                  except requests.exceptions.JSONDecodeError:\n",
        "                      logger.error(f\"Query '{query}' returned status 200 but response was not valid JSON. Body\")\n",
        "                      return {}\n",
        "                # If no results, the loop will continue to the next attempt.\n",
        "                #logger.warning(f\"Query '{query}' ANSWER Query returned no results on attempt {attempt + 1}/{max_attempts}.\")\n",
        "\n",
        "            else:\n",
        "                logger.error(\n",
        "                    f\"API call for query '{query}' failed with status {search_response.status_code}: {search_response.reason}\\n\"\n",
        "                    f\"Response Body: {search_response.text}\"\n",
        "                )\n",
        "                return {}\n",
        "\n",
        "        except (requests.exceptions.RequestException, NameError) as e:\n",
        "            logger.error(f\"An error occurred during the search process: {e}\")\n",
        "            # Return an empty dictionary to allow the main loop to continue safely\n",
        "            return {}\n",
        "\n",
        "        # If this is not the last attempt, wait before retrying.\n",
        "        if attempt < max_attempts - 1:\n",
        "            backoff = (attempt+1) * 2 * (attempt+1)\n",
        "            time.sleep(backoff) # exponentailly back of retry\n",
        "\n",
        "\n",
        "    # This line is reached only if all attempts failed to find results.\n",
        "    #logger.error(f\"Query '{query}' failed to retrieve results after {max_attempts} attempts.\")\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yw_QzjrVbEvY",
      "metadata": {
        "id": "yw_QzjrVbEvY"
      },
      "source": [
        "## 6.3 Get Full Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d394de-864b-4bf0-992d-5c1038affd1d",
      "metadata": {
        "id": "63d394de-864b-4bf0-992d-5c1038affd1d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import datetime\n",
        "\n",
        "# Assuming get_search_results() and get_answer_results() are defined correctly\n",
        "\n",
        "def get_full_response_dict(query: str, K: int) -> dict:\n",
        "    \"\"\"\n",
        "    Processes a query to get search and answer results and returns them in a dictionary.\n",
        "    \"\"\"\n",
        "    # --- Step 0: Initialize Empty ---\n",
        "    attributionToken=None,\n",
        "    structured_search_results=None,\n",
        "    answer_data={}\n",
        "\n",
        "    # --- Step 1: Get session info from a search call ---\n",
        "    search_data, search_attempts = get_search_results(query, K)\n",
        "    if search_data:\n",
        "      #print(\"search data found running answer section now\")\n",
        "      attributionToken = search_data.get('attributionToken')\n",
        "                  # Extract all result links\n",
        "      session_info = search_data.get('sessionInfo', {})\n",
        "      session_name = session_info.get('name')\n",
        "      queryId = session_info.get('queryId')\n",
        "      response_urls = []\n",
        "      relevance_scores = []\n",
        "      structured_search_results = []\n",
        "      for result in search_data.get('results', []):\n",
        "          link_struct={}\n",
        "          link_id = link = result.get('document', {}).get('id', {})\n",
        "          link = result.get('document', {}).get('derivedStructData', {}).get('link')\n",
        "          values_list = result.get('modelScores', {}).get('relevance_score', {}).get('values', [])\n",
        "          relevance_score = values_list[0] if values_list else None\n",
        "\n",
        "\n",
        "          if link:\n",
        "              link_struct = {\n",
        "                  \"id\" : link_id,\n",
        "                  \"link\": link,\n",
        "                  \"relevance_score\": relevance_score\n",
        "              }\n",
        "              structured_search_results.append(link_struct)\n",
        "          else:\n",
        "              link_struct = {\n",
        "                  \"id\": link_id,\n",
        "                  \"link\": \"no link\",\n",
        "                  \"relevance_score\": \"no score\"\n",
        "              }\n",
        "              structured_search_results.append(link_struct)\n",
        "\n",
        "      # --- Step 2: Get the generative answer with retries ---\n",
        "      if queryId and session_name:\n",
        "          answer_data = {}  # Default to an empty dict\n",
        "          max_attempts = 3\n",
        "\n",
        "          for attempt in range(max_attempts):\n",
        "              #print(f\"Running answer (Attempt {attempt + 1}/{max_attempts})...\")\n",
        "              try:\n",
        "                  # Attempt to call the function which might raise an error\n",
        "                  temp_data = get_answer_results(query, queryId, session_name)\n",
        "\n",
        "                  # If the function returns valid data, it's a success\n",
        "                  if temp_data:\n",
        "                      answer_data = temp_data\n",
        "                      #print(\"Successfully retrieved answer.\")\n",
        "                      break # Exit the loop on success\n",
        "                  else:\n",
        "                      # This handles the case where the call works but returns no data\n",
        "                      print(\"Call successful, but no answer data was returned.\")\n",
        "\n",
        "              except IndexError:\n",
        "                  # This specifically catches the 'list index out of range' error\n",
        "                  print(f\"Caught an 'IndexError'. This is likely due to unexpected API response format.\")\n",
        "\n",
        "              # --- Retry Logic ---\n",
        "              # If we've reached this point, it means the attempt failed (either by error or empty data).\n",
        "              # Check if it's not the last attempt before waiting.\n",
        "              if attempt < max_attempts - 1:\n",
        "                  print(f\"Retrying in 5 seconds...\")\n",
        "                  time.sleep(5)\n",
        "              else:\n",
        "                  print(\"All attempts failed. Proceeding without answer data.\")\n",
        "\n",
        "      else:\n",
        "          print(\"Warning: Could not find session_name or queryId to get an answer.\")\n",
        "          answer_data = {}\n",
        "\n",
        "    # --- Step 3: Extract all data points and build the final dictionary ---\n",
        "\n",
        "    # First, create a base dictionary with the search data you already have.\n",
        "    # Set defaults for the answer fields.\n",
        "    final_output = {\n",
        "        'attributionToken': attributionToken,\n",
        "        'searchResults': structured_search_results,\n",
        "        'answerText': None,\n",
        "        'answerQueryToken': None\n",
        "    }\n",
        "\n",
        "    # Next, if you successfully found answer data, update the dictionary.\n",
        "    if answer_data:\n",
        "        #print(\"Answer data found. Adding to final output.\")\n",
        "        # Safely get the nested 'answerText'\n",
        "        answer_details = answer_data.get('answer', {})\n",
        "        final_output['answerText'] = answer_details.get('answerText')\n",
        "        final_output['answerQueryToken'] = answer_data.get('answerQueryToken')\n",
        "    else:\n",
        "        #print(\"Failed to get a valid answer from the API after all retries.\")\n",
        "        final_output['answerText'] = f\"Failed to get an answer\"\n",
        "\n",
        "    # Finally, return the dictionary you've built.\n",
        "    # This ensures a dictionary is always returned if the initial search was successful.\n",
        "    return final_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FYJR-DZUbZ8J",
      "metadata": {
        "id": "FYJR-DZUbZ8J"
      },
      "source": [
        "# 7.0 Main Processing Loop - Get Answers with Top K Search Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1494bd2c-a95d-4ef6-bf73-124505030a7c",
      "metadata": {
        "id": "1494bd2c-a95d-4ef6-bf73-124505030a7c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from google.cloud import storage\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "# Assuming 'get_full_response_dict' might raise standard network exceptions\n",
        "# If it raises custom errors, add them to the tuple, e.g., (requests.exceptions.RequestException, MyCustomApiError)\n",
        "from requests.exceptions import RequestException\n",
        "\n",
        "# --- Configuration ---\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "# --- 1. Define a single Evaluation Run ID at the beginning ---\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "assist_run_id = f\"{connector}_{timestamp}\"\n",
        "print(f\"🚀 Starting new run. Search & Assist Run with ID: {assist_run_id}\")\n",
        "\n",
        "# --- Define GCS paths ---\n",
        "TEMP_GCS_FOLDER = f\"{folder}/temp_batches_{assist_run_id}\" # Temp folder is now unique to the run\n",
        "TEMP_GCS_DIR_URI = f\"gs://{bucket}/{TEMP_GCS_FOLDER}\"\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def process_row_with_retry(row):\n",
        "    \"\"\"\n",
        "    Function to process a single row of the DataFrame, now with a retry mechanism.\n",
        "    \"\"\"\n",
        "    #print(f\"Attempting to process query: {row['search_query']}...\")\n",
        "    try:\n",
        "        query = row['search_query']\n",
        "        # The API call is the part that might fail and needs retrying\n",
        "        api_results_dict = get_full_response_dict(query, K)\n",
        "\n",
        "        output_data = row.to_dict()\n",
        "\n",
        "        output_data.update(api_results_dict)\n",
        "\n",
        "        return output_data\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process query '{row['search_query']}' after all retries. Error: {e}\")\n",
        "        # Re-raise the exception to stop the process or handle it as needed\n",
        "        raise\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main Processing Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Starting Search Query Processing in Batches (GCS) ---\")\n",
        "\n",
        "individual_gcs_paths = []\n",
        "tqdm.pandas(desc=\"Processing queries\")\n",
        "\n",
        "# 2. Split the DataFrame into batches\n",
        "list_of_batches = np.array_split(golden_data, len(golden_data) // BATCH_SIZE + 1)\n",
        "total_batches = len(list_of_batches)\n",
        "print(f\"Processing {len(golden_data)} rows in {total_batches} batches...\")\n",
        "\n",
        "# 3. Loop through and process each batch\n",
        "for i, batch_df in enumerate(list_of_batches):\n",
        "    current_batch_num = i + 1\n",
        "    print(f\"\\n--- Processing Batch {current_batch_num}/{total_batches} ---\")\n",
        "\n",
        "    if batch_df.empty:\n",
        "        print(\"Skipping empty batch.\")\n",
        "        continue\n",
        "\n",
        "    # A. Apply the processing function\n",
        "    results_list = batch_df.progress_apply(process_row_with_retry, axis=1).tolist()\n",
        "    processed_batch_df = pd.DataFrame(results_list)\n",
        "\n",
        "    # B. Add the consistent eval_run_id to the batch DataFrame\n",
        "    processed_batch_df['assist_run_id'] = assist_run_id\n",
        "\n",
        "    # C. Write batch results to a temporary CSV file in GCS\n",
        "    batch_gcs_uri = f\"{TEMP_GCS_DIR_URI}/batch_{current_batch_num}.csv\"\n",
        "    print(f\"Saving batch results to: {batch_gcs_uri}\")\n",
        "    processed_batch_df.to_csv(batch_gcs_uri, index=False)\n",
        "    individual_gcs_paths.append(batch_gcs_uri)\n",
        "    print(f\"✅ Batch {current_batch_num} saved to GCS.\")\n",
        "\n",
        "# 4. --- Merge, Save Final File, and Clean Up GCS ---\n",
        "print(\"\\n--- Finalizing Results from GCS ---\")\n",
        "\n",
        "if not individual_gcs_paths:\n",
        "    print(\"Warning: No batch files were created. Final DataFrame is empty.\")\n",
        "else:\n",
        "    # A. Merge all individual CSVs directly from GCS\n",
        "    print(f\"Merging {len(individual_gcs_paths)} temporary CSV files from GCS...\")\n",
        "    evaluation_df = pd.concat(\n",
        "        (pd.read_csv(gcs_uri) for gcs_uri in individual_gcs_paths),\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "    # B. Save the final merged DataFrame to its final destination in GCS\n",
        "    # The output filename now includes the run ID for better tracking\n",
        "    final_output_file_name = f\"{output_file_name}_{assist_run_id}.csv\"\n",
        "    final_gcs_uri = f\"gs://{bucket}/{folder}/{final_output_file_name}\"\n",
        "\n",
        "    print(f\"\\nSaving final merged results to GCS path: {final_gcs_uri}...\")\n",
        "    evaluation_df.to_csv(final_gcs_uri, index=False)\n",
        "    print(f\"✅ Successfully saved final results to GCS: {final_gcs_uri}\")\n",
        "\n",
        "    # C. Clean up by deleting the temporary files from GCS\n",
        "    print(f\"\\nCleaning up temporary files from {TEMP_GCS_DIR_URI}...\")\n",
        "    try:\n",
        "        storage_client = storage.Client(project=project)\n",
        "        bucket = storage_client.bucket(bucket)\n",
        "        blobs_to_delete = list(bucket.list_blobs(prefix=TEMP_GCS_FOLDER))\n",
        "\n",
        "        if blobs_to_delete:\n",
        "            bucket.delete_blobs(blobs_to_delete)\n",
        "            print(f\"✅ Successfully deleted {len(blobs_to_delete)} temporary files from GCS.\")\n",
        "        else:\n",
        "            print(\"No temporary files found to delete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GCS cleanup: {e}\")\n",
        "\n",
        "print(\"\\n--- Cloud-based batch processing complete. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vXS7JhPV1eOF",
      "metadata": {
        "id": "vXS7JhPV1eOF"
      },
      "source": [
        "# 8.0 Answer Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4FdvqhTZb9IR",
      "metadata": {
        "id": "4FdvqhTZb9IR"
      },
      "source": [
        "## 8.1. Defintion of Evaluation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7f1ddd35-2bfc-4747-be52-41b86d5d0515",
      "metadata": {
        "id": "7f1ddd35-2bfc-4747-be52-41b86d5d0515"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from google import genai\n",
        "import traceback\n",
        "\n",
        "\n",
        "# This is the prompt to validate the quality of the answer results and rate it on a scale\n",
        "EVALUATION_PROMPT = \"\"\"\n",
        "You are a meticulous and objective expert evaluator. Your task is to compare a generated `Answer` to a ground-truth `Expected Answer`, strictly considering the context of the original `Query`. Your evaluation must determine if the `Expected Answer` is factually and semantically contained within the generated `Answer`.\n",
        "\n",
        "You must rate the match on the scale of 0 to 5 defined below. Your response must be ONLY a single, valid JSON object with two keys: \"rating\" (an integer from 0-5) and \"justification\" (a brief string, no more than 1-2 sentences, explaining your rating based on the scale). Do not add any text before or after the JSON object.\n",
        "\n",
        "---\n",
        "[RATING SCALE DEFINITION]\n",
        "\n",
        "- **5 (Excellent Match):** The `Answer` perfectly contains all the information from the `Expected Answer`. It is factually correct, complete, and concise, with no significant missing or extraneous information.\n",
        "\n",
        "- **4 (Very Good Match):** The `Answer` contains all the key information from the `Expected Answer` but may include minor, harmless additional details or have slightly different phrasing that doesn't alter the meaning.\n",
        "\n",
        "- **3 (Good/Partial Match):** The `Answer` contains the main point of the `Expected Answer` but is missing some important secondary details, or it contains a notable amount of extra, less relevant information.\n",
        "\n",
        "- **2 (Fair/Minor Match):** The `Answer` contains some correct elements from the `Expected Answer` but misses the primary point or is factually incomplete in a significant way.\n",
        "\n",
        "- **1 (Poor Match):** The `Answer` touches on the topic of the `Query` but provides information that is mostly irrelevant or fails to align with the `Expected Answer`.\n",
        "\n",
        "- **0 (No Match):** The `Answer` contains no relevant or correct information from the `Expected Answer`. It is completely off-topic, factually wrong, or a refusal to answer.\n",
        "---\n",
        "\n",
        "[TASK]\n",
        "Query: \"{query}\"\n",
        "Expected Answer: \"{expected_answer}\"\n",
        "Answer: \"{answer}\"\n",
        "\n",
        "Your Response:\n",
        "\"\"\"\n",
        "\n",
        "# This is the prompt to validate if the golden link is found in the search link results\n",
        "LINK_PROMPT =\"\"\"\n",
        "Your one and only task is to respond with either TRUE or FALSE, and nothing else.\n",
        "\n",
        "You are a meticulous and objective expert evaluator. Your task is to compare a generated `searchResults` to a ground-truth `goldenLink`. The 'goldenLink' contains a url to file in gs starting with gs://. Try to find if this url is present within the search results.\n",
        "\n",
        "The links do not have to match exactly, but the document name must match. Your final response must be the word TRUE or FALSE, and nothing else.\n",
        "----\n",
        "[Example]\n",
        "gs://bucketabc/my%20important%20document%20.pdf matches: \"my important document.pdf\"\n",
        "-----\n",
        "[Expected Answer]\n",
        "[TRUE;FALSE]\n",
        "\n",
        "------\n",
        "[TASK]\n",
        "goldenLink: \"{goldenLink}\"\n",
        "searchResults: \"{searchResults}\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Initialize the Model\n",
        "client = genai.Client(vertexai=True, project=project, location=vertex_region)\n",
        "MODEL_ID = \"gemini-2.5-flash\"\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_answer(row):\n",
        "    \"\"\"\n",
        "    Calls Gemini to evaluate an answer.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        answer = str(row['answerText'])\n",
        "        query = str(row['search_query'])\n",
        "        expected_answer = str(row['expected_answer'])\n",
        "\n",
        "        prompt = EVALUATION_PROMPT.format(\n",
        "            query=query,\n",
        "            expected_answer=expected_answer,\n",
        "            answer=answer\n",
        "        )\n",
        "        # The GenerationConfig class is accessed from `genai` now\n",
        "\n",
        "                # Instead of a genai.GenerationConfig object, use a simple dictionary.\n",
        "        # 1. Use a dictionary for the configuration.\n",
        "        generation_config_dict = {\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"temperature\": 0\n",
        "        }\n",
        "\n",
        "        # 2. Use your original, correct method call, passing the dictionary to the 'config' parameter.\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=prompt,\n",
        "            config=generation_config_dict\n",
        "        )\n",
        "\n",
        "        # This check is crucial to handle blocked responses\n",
        "        if not response.candidates:\n",
        "            justification = f\"Error: Model returned an empty or blocked response. Reason: {response.prompt_feedback}\"\n",
        "            print(justification) # Print error for debugging\n",
        "            return pd.Series({'rating': -1, 'justification': justification})\n",
        "\n",
        "        result_dict = json.loads(response.text)\n",
        "\n",
        "        # This is the successful return path\n",
        "        return pd.Series({\n",
        "            'rating': int(result_dict.get('rating', -1)),\n",
        "            'justification': str(result_dict.get('justification', ''))\n",
        "        })\n",
        "    except Exception as e:\n",
        "        # This general except block catches ALL other errors and ensures a Series is returned\n",
        "        error_message = f'An unexpected error occurred: {e}'\n",
        "        print(error_message) # Print error for debugging\n",
        "        return pd.Series({'rating': -1, 'justification': error_message})\n",
        "\n",
        "\n",
        "def evaluate_link(row):\n",
        "    \"\"\"\n",
        "    Calls Gemini to evaluate links and returns the full error if one occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        goldenLink = str(row['golden_url'])\n",
        "        search_results_list = str(row['searchResults'])\n",
        "\n",
        "        prompt = LINK_PROMPT.format(\n",
        "            goldenLink=goldenLink,\n",
        "            searchResults=search_results_list,\n",
        "        )\n",
        "        generation_config_dict = {\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"temperature\": 0\n",
        "        }\n",
        "\n",
        "        # 2. Use your original, correct method call, passing the dictionary to the 'config' parameter.\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=prompt,\n",
        "            config=generation_config_dict\n",
        "        )\n",
        "\n",
        "        return pd.Series({\n",
        "            'link_evaluation': response.text,\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"--- An error occurred. Capturing full traceback. ---\")\n",
        "        full_error_details = traceback.format_exc()\n",
        "        print(full_error_details)\n",
        "        return pd.Series({\n",
        "            'link_evaluation': None\n",
        "        })\n",
        "\n",
        "\n",
        "def evaluate_row_fully(row):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    A wrapper function that calls both evaluate_answer and evaluate_link\n",
        "    for a single row and combines their results into one Series.\n",
        "    \"\"\"\n",
        "    # Call the first evaluation function\n",
        "    answer_result = evaluate_answer(row)\n",
        "\n",
        "    # Call the second evaluation function\n",
        "    link_result = evaluate_link(row)\n",
        "\n",
        "    # Rename the outputs to prevent column name collisions.\n",
        "    answer_result = answer_result.rename({\n",
        "        'rating': 'answer_rating',\n",
        "        'justification': 'answer_justification'\n",
        "    })\n",
        "\n",
        "    # Combine the results from both functions into a single pandas Series\n",
        "    full_result = pd.concat([answer_result, link_result])\n",
        "\n",
        "    return full_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xKAVLYHG4dZG",
      "metadata": {
        "id": "xKAVLYHG4dZG"
      },
      "source": [
        "## 8.2 Load the Evaluation Data from File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d37de2af-812f-4140-ab58-f50167c13419",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d37de2af-812f-4140-ab58-f50167c13419",
        "outputId": "f51c80bb-4060-45be-d278-63a3165fc680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/eval_output_gcs_test_20250813_221500.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "evaluation_df =[]\n",
        "#Uncomment this line, if a specific file should be used for the evaluation\n",
        "final_output_file_name = 'eval_output_gcs_test_20250813_221500'\n",
        "\n",
        "\n",
        "# 2. Construct the full GCS URI for the output file.\n",
        "input_file = f\"{final_output_file_name}.csv\"\n",
        "input_gcs_uri = f\"gs://{bucket}/{folder}/{input_file}\"\n",
        "\n",
        "try:\n",
        "    evaluation_df =  pd.read_csv(\n",
        "    input_gcs_uri\n",
        ")\n",
        "    print(f\"Successfully loaded {input_gcs_uri}.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'search_eval.csv' not found. Please ensure the file from the previous step exists.\")\n",
        "    # Create a dummy dataframe for demonstration if file not found\n",
        "    evaluation_df = pd.DataFrame({\n",
        "        'answer': [\"Gemini is a family of multimodal models.\", \"The sky is blue due to Rayleigh scattering.\"],\n",
        "        'expected_answer': [\"Gemini is a family of models.\", \"The sky appears blue because of how the atmosphere interacts with sunlight.\"]\n",
        "    })\n",
        "#print(evaluation_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DeZQUKCu1kD-",
      "metadata": {
        "id": "DeZQUKCu1kD-"
      },
      "source": [
        "## 8.3 Test the evluation with a single row\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "import google.auth\n",
        "\n",
        "from google.colab import userdata # <-- Import userdata to access secrets\n",
        "\n",
        "\n",
        "# --- 2. Prepare the Data for the SDK ---\n",
        "test_data = {\n",
        "    'search_query': [\"what is Gemini?\"],\n",
        "    'expected_answer': [\"Gemini is magic\"],\n",
        "    'answer': [\"Gemini is a AI model made by Google\"],\n",
        "    'golden_url' : [[\"https://somerandomlink.com/type/1234/thegoldendocument.csv\"]],\n",
        "    'searchResults' : [[\n",
        "        {'source': 'gs://wikipedia_source_bucket1213092/wikipedia-articles/2002 US Open  Womens singles qualifying_a96b60.pdf'},\n",
        "        {'source': 'gs://wikipedia_source_bucket1213092/wikipedia-articles/thegoldendocument.csv'},\n",
        "        {'source': 'gs://wikipedia_source_bucket1213092/wikipedia-articles/2022 European Speed Skating Championships  Womens team sprint_abd5c1.pdf'}\n",
        "    ]]\n",
        "}\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "print(\"\\n--- Running evaluation on the single test row ---\")\n",
        "test_row = test_df.iloc[0]\n",
        "\n",
        "# --- 3. Create and Run the Evaluation Task ---\n",
        "print(\"\\n--- Defining and running the evaluation task ---\")\n",
        "\n",
        "\n",
        "evaluation_result = evaluate_row_fully(test_row)\n",
        "\n",
        "# --- 4. Display the Results ---\n",
        "print(\"\\n--- Evaluation Result ---\")\n",
        "# The result object has a convenient method to convert it to a DataFrame\n",
        "\n",
        "print(evaluation_result)\n"
      ],
      "metadata": {
        "id": "Qcs3iRrbd3Hi"
      },
      "id": "Qcs3iRrbd3Hi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "YINFyP_1cPfq",
      "metadata": {
        "id": "YINFyP_1cPfq"
      },
      "source": [
        "## 8.4 Run the evaluation and save output to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af369a9-444c-47f9-8c1d-e67ec7bdbe58",
      "metadata": {
        "id": "9af369a9-444c-47f9-8c1d-e67ec7bdbe58"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "BATCH_SIZE = 20\n",
        "TEMP_BATCH_DIR = \"temp_evaluation_batches\" # Local temporary directory for batch CSVs\n",
        "\n",
        "\n",
        "# define the eval run id\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "evalDate = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
        "eval_run_id = f\"{connector}_{timestamp}\"\n",
        "\n",
        "# --- Main Evaluation Loop ---\n",
        "print(\"\\n--- Starting AI-based evaluation of answers in batches ---\")\n",
        "\n",
        "# Ensure the necessary column exists before proceeding\n",
        "if 'expected_answer' not in evaluation_df.columns:\n",
        "    print(\"Error: 'expected_answer' column not found. Stopping evaluation.\")\n",
        "else:\n",
        "    # 1. Setup for batch processing\n",
        "    os.makedirs(TEMP_BATCH_DIR, exist_ok=True) # Create temp dir for batch files\n",
        "    individual_csv_paths = [] # To store paths of batch CSVs for final merge\n",
        "    tqdm.pandas(desc=\"Evaluating answer quality\")\n",
        "\n",
        "    # Split the DataFrame into batches\n",
        "    list_of_batches = np.array_split(evaluation_df, len(evaluation_df) // BATCH_SIZE + 1)\n",
        "    total_batches = len(list_of_batches)\n",
        "\n",
        "    print(f\"Processing {len(evaluation_df)} rows in {total_batches} batches of up to {BATCH_SIZE} rows each.\")\n",
        "\n",
        "    # 2. Loop through each batch\n",
        "    for i, batch_df in enumerate(list_of_batches):\n",
        "        current_batch_num = i + 1\n",
        "        print(f\"\\n--- Processing Batch {current_batch_num}/{total_batches} ---\")\n",
        "\n",
        "        if batch_df.empty:\n",
        "            print(\"Skipping empty batch.\")\n",
        "            continue\n",
        "\n",
        "        # A. Apply the evaluation function to the current batch\n",
        "        # Using .progress_apply for a progress bar within the batch\n",
        "        batch_results = batch_df.progress_apply(evaluate_row_fully, axis=1)\n",
        "\n",
        "        if isinstance(batch_results, pd.Series):\n",
        "            batch_results = batch_results.to_frame().T\n",
        "\n",
        "        # B. Join results back to the batch DataFrame\n",
        "        # Define columns that might already exist and need to be replaced\n",
        "        cols_to_replace = ['answer_rating', 'answer_justification', 'link_evaluation']\n",
        "        existing_cols_to_drop = [col for col in cols_to_replace if col in batch_df.columns]\n",
        "\n",
        "        if existing_cols_to_drop:\n",
        "            batch_df = batch_df.drop(columns=existing_cols_to_drop)\n",
        "\n",
        "        processed_batch_df = batch_df.join(batch_results)\n",
        "\n",
        "        # C. Add metadata\n",
        "        processed_batch_df['connector'] = connector\n",
        "        processed_batch_df['evaluationRunId'] = eval_run_id\n",
        "        processed_batch_df['timestamp'] = evalDate\n",
        "\n",
        "        # D. Write individual batch results to a local CSV file\n",
        "        batch_file_path = os.path.join(TEMP_BATCH_DIR, f\"batch_{current_batch_num}.csv\")\n",
        "        processed_batch_df.to_csv(batch_file_path, index=False)\n",
        "        individual_csv_paths.append(batch_file_path)\n",
        "        print(f\"✅ Batch {current_batch_num} saved locally to: {batch_file_path}\")\n",
        "\n",
        "        # E. Append results to BigQuery\n",
        "        table_name = f\"{connector}_validation\"\n",
        "        destination_table = f\"{output_dataset}.{table_name}\"\n",
        "\n",
        "        print(f\"Appending batch {current_batch_num} to BigQuery table: {project}.{destination_table}...\")\n",
        "        processed_batch_df.to_gbq(\n",
        "            destination_table=destination_table,\n",
        "            project_id=project,\n",
        "            if_exists='append', # Append results for each batch\n",
        "            progress_bar=True\n",
        "        )\n",
        "        print(f\"✅ Successfully appended batch {current_batch_num} to BigQuery.\")\n",
        "\n",
        "    # 3. --- Post-Loop: Merge individual CSVs and Clean Up ---\n",
        "    print(\"\\n--- Finalizing Results ---\")\n",
        "\n",
        "    # A. Merge all individual CSVs into one DataFrame\n",
        "    if not individual_csv_paths:\n",
        "        print(\"Warning: No batch CSV files were created to merge.\")\n",
        "    else:\n",
        "        print(f\"Merging {len(individual_csv_paths)} individual CSV files...\")\n",
        "        merged_df = pd.concat((pd.read_csv(f) for f in individual_csv_paths), ignore_index=True)\n",
        "\n",
        "        # B. Save the final merged CSV to Google Cloud Storage\n",
        "        output_file = f\"{output_file_name}.csv\"\n",
        "        output_gcs_uri = f\"gs://{bucket}/{folder}/{output_file}\"\n",
        "\n",
        "        print(f\"Saving final merged CSV to GCS path: {output_gcs_uri}...\")\n",
        "        merged_df.to_csv(output_gcs_uri, index=False)\n",
        "        print(f\"✅ Successfully saved merged results to GCS: {output_gcs_uri}\")\n",
        "\n",
        "        # C. Clean up by deleting the local temporary files and directory\n",
        "        print(\"Cleaning up temporary batch files...\")\n",
        "        for file_path in individual_csv_paths:\n",
        "            try:\n",
        "                os.remove(file_path)\n",
        "            except OSError as e:\n",
        "                print(f\"Error deleting file {file_path}: {e}\")\n",
        "        try:\n",
        "            os.rmdir(TEMP_BATCH_DIR)\n",
        "            print(f\"✅ Successfully removed temporary directory: {TEMP_BATCH_DIR}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error removing directory {TEMP_BATCH_DIR}: {e}\")\n",
        "\n",
        "print(\"\\n--- Evaluation process complete. ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m131",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

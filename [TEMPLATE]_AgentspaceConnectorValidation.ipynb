{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxbilling/AgentspaceEvaluation/blob/main/%5BTEMPLATE%5D_AgentspaceConnectorValidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9769c2eb-6dcf-40b2-971d-8138b565ec0a",
      "metadata": {
        "id": "9769c2eb-6dcf-40b2-971d-8138b565ec0a"
      },
      "source": [
        "Author: @maxbilling\n",
        "Based on: go/agentspace-eval-customer\n",
        "\n",
        "Run Search Quality and Generated Answer Quality Evaluations\n",
        "This notebook helps you to automate running sevaral search queries. The generated reponse to the question and top K search results are saved to a file for further review and human assessment.\n",
        "Steps:\n",
        "\n",
        "Create a csv file with columns:\n",
        "\n",
        "*   'search_query'\n",
        "*   'expected_answer'\n",
        "*   'golden_url'\n",
        "\n",
        "\n",
        "***'golden_url'***: Add a link to the golden response if you have one. You can also provide multiple links separated by a new line. This is for future evaluation to autogenerate metrics\n",
        "\n",
        "Durin evaluation the following steps are performed:\n",
        "5.0 search queries are imported from the file\n",
        "6.1 a semantic search is performed with the default_search:search configuration of your agentspace app\n",
        "6.2 based on the serach session a answer is genreated with the default_search:aswer configuraiton of your agentspace app\n",
        "6.3 both results are combined the results are written back to the file\n",
        "\n",
        "After the search and aswer results are written a evaluation takes place with\n",
        "\n",
        "8.2. search and answer results are loaded from file\n",
        "8.3. one by one evaluated by two models defined in 8.1. One model to evaluate the quality of the answer the second model to evaluate if the golden_url was part of the urls found.\n",
        "8.4 the results are saved to file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E16fqffhZe_x",
      "metadata": {
        "id": "E16fqffhZe_x"
      },
      "source": [
        "# 1.0 Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b095a2-2ad7-4531-add1-a83213718223",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83b095a2-2ad7-4531-add1-a83213718223",
        "outputId": "c9a38da5-d30b-4a09-e0f2-6f80dc32ac89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.178.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet colabtools\n",
        "!pip install --quiet requests-toolbelt==1.0.0\n",
        "!pip install --quiet google-cloud-discoveryengine\n",
        "!pip install --upgrade --user --quiet google-cloud-aiplatform\n",
        "!pip install --upgrade --quiet  langchain-core langchain-google-vertexai\n",
        "!pip install --upgrade --quiet json5\n",
        "!pip install --quiet scipy\n",
        "!pip install --quiet gspread\n",
        "!pip install --quiet --upgrade pandas pyarrow pandas-gbq\n",
        "!pip install --quiet upgrade google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OF0gvLdCZo9x",
      "metadata": {
        "id": "OF0gvLdCZo9x"
      },
      "source": [
        "# 2.0 Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f4499c-e6d6-4cd3-b3e6-9ed571a6f633",
      "metadata": {
        "id": "21f4499c-e6d6-4cd3-b3e6-9ed571a6f633"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import logging\n",
        "import datetime\n",
        "import requests\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "\n",
        "from google.auth import default\n",
        "from google.colab import auth\n",
        "import google.auth.transport.requests\n",
        "import vertexai\n",
        "import gspread\n",
        "\n",
        "# Logger Setup\n",
        "logger = logging.getLogger()\n",
        "logging.basicConfig(filename='spark_eval_notebook_logs.log', filemode='a', level=logging.DEBUG)\n",
        "\n",
        "creds, _ = google.auth.default()\n",
        "auth_req = google.auth.transport.requests.Request()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DE4CFo_WZ6N8",
      "metadata": {
        "id": "DE4CFo_WZ6N8"
      },
      "source": [
        "# 3.0 Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985f0f6e-6a2b-455d-98d9-dc20e1cd94ea",
      "metadata": {
        "id": "985f0f6e-6a2b-455d-98d9-dc20e1cd94ea"
      },
      "outputs": [],
      "source": [
        "# Use project number and not Project ID\n",
        "connector= \"gcs_test\" # @param {\"type\":\"string\",\"placeholder\":\"e.g. 1234567890\"}\n",
        "project_num = \"694598770214\" # @param {\"type\":\"string\",\"placeholder\":\"e.g. 1234567890\"}\n",
        "project = \"maxbproject\" # @param {\"type\":\"string\",\"placeholder\":\"e.g. 1234567890\"}\n",
        "# Engine ID is the ID of the Agent builder APP\n",
        "engine_id = \"wikipedia-agentspace_1753780981307\" # @param {\"type\":\"string\",\"placeholder\":\"e.g. my-search-app_12345\"}\n",
        "\n",
        "# Assist or Search - Note: \"Assist\" means \"Search + Assist\"\n",
        "# Whereas Search means \"Search + Answer\"\n",
        "app_type = \"assist\" # @param [\"search\",\"assist\"]\n",
        "\n",
        "# Location\n",
        "location = 'eu' # @param {\"type\":\"string\",\"placeholder\":\"Set: \\\"global\\\" \"}\n",
        "\n",
        "# evaluation region, global is not supported yet\n",
        "evaluation_region = \"eu\" # @param {\"type\":\"string\",\"placeholder\":\"evaluation region, global is not supported yet\"}\n",
        "vertex_region = 'europe-west1' # @param {\"type\":\"string\",\"placeholder\":\"Set: \\\"global\\\" \"}\n",
        "# Use Project ID\n",
        "auth_project_id = 'maxbproject' # @param {\"type\":\"string\",\"placeholder\":\"e.g. my-project-id\"}\n",
        "\n",
        "# Input Queries\n",
        "# Note: Every user needs to have their own copy of this doc. Please make a copy of the golden data google sheet below and add that link.\n",
        "# two columns named search_query, expected_answer\n",
        "bucket = \"wikipedia_source_bucket1213092\" # @param {\"type\":\"string\",\"placeholder\":\"Google Sheets file URL\"}\n",
        "folder = \"wikipedia-articles-test-2\" # @param {\"type\":\"string\",\"placeholder\":\"Google Sheets file URL\"}\n",
        "output_file_name = 'eval_output' # @param {\"type\":\"string\",\"placeholder\":\"e.g. test_output1\"}\n",
        "\n",
        "output_dataset= 'agentspace_validation' # @param {\"type\":\"string\",\"placeholder\":\"e.g. dataset\"}\n",
        "# Input Queries - Note: Queries tab name within Google Sheets file\n",
        "input_file_name = 'input_queries' # @param {\"type\":\"string\",\"placeholder\":\"e.g. 'input_queries'\"}\n",
        "\n",
        "# (Optional) Output file name used for debugging. This file will be saved in the colab env.\n",
        "\n",
        "# Eval data worksheet\n",
        "sheet_name_suffix = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "do_evaluation = True # @param {\"type\":\"boolean\",\"placeholder\":\"run evaluation\"}\n",
        "\n",
        "\n",
        "WORKFORCE_POOL_ID=\"\" # @param {\"type\":\"string\", \"placeholder\":\"pool id (only if using wif)\"}\n",
        "WORKFORCE_PROVIDER_ID=\"\" # @param {\"type\":\"string\", \"placeholder\":\"provider id (only if using wif)\"}\n",
        "WIF_CONFIG_FILE=\"my_wif_auth_config.json\"\n",
        "\n",
        "# session user pseudo id\n",
        "session_user_pseudo_id = \"testuser01\" # @param {\"type\":\"string\",\"placeholder\":\"session user pseudo id\"}\n",
        "\n",
        "# Top 'K' for search Results\n",
        "K = \"3\" # @param {\"type\":\"string\"}\n",
        "\n",
        "serving_config_id = \"default_search\" # @param {\"type\":\"string\",\"placeholder\":\"default_search\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sk1PEy89aLxR",
      "metadata": {
        "id": "Sk1PEy89aLxR"
      },
      "source": [
        "# 4.0 Authenticate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Vzrf4EiaSK_",
      "metadata": {
        "id": "3Vzrf4EiaSK_"
      },
      "source": [
        "\n",
        "## 4.1 glcoud - Google User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "9a9845c9-27c1-46f7-b9c1-0bffd539b70e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a9845c9-27c1-46f7-b9c1-0bffd539b70e",
        "outputId": "708726be-f870-40e1-ba88-2c82e99712af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\n",
            "Quota project \"maxbproject\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "\n",
        "\n",
        "!gcloud auth application-default set-quota-project {project}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2YFvxEkOruwT",
      "metadata": {
        "id": "2YFvxEkOruwT"
      },
      "source": [
        "## 4.2 glcoud - Workforce Identity Federated User"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PTcX2K5Lrtps",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTcX2K5Lrtps",
        "outputId": "7ca4cea7-2679-429c-99c8-da0b6f6d75f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=HzEactqXtP8kHDlKjbbr95kVrKvVBe&prompt=consent&token_usage=remote&access_type=offline&code_challenge=MUeZotEQCwp95ERWAXwyKMW6f64nf0pqu7s7CR0qeco&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AVMBsJhplv6CvSZ6NisdykWzBeq241b5u6N4LB33ce1r5SRYwhNsLvVu3iMAFeroruKp4g\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ],
      "source": [
        "!gcloud iam workforce-pools create-login-config \\\n",
        "    locations/global/workforcePools/{WORKFORCE_POOL_ID}/providers/{WORKFORCE_PROVIDER_ID} \\\n",
        "    --output-file={WIF_CONFIG_FILE}\n",
        "!gcloud auth login --no-launch-browser --login-config={WIF_CONFIG_FILE}\n",
        "!gcloud auth application-default set-quota-project {project}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ImQZc0vmtlX",
      "metadata": {
        "id": "4ImQZc0vmtlX"
      },
      "source": [
        "## 4.1 Obtain Token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "4e5ca97b-0830-426e-8a08-14d7c5a44690",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e5ca97b-0830-426e-8a08-14d7c5a44690",
        "outputId": "bca128be-256f-4ab7-cfca-d5c550388c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
            "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Access Token: ya29.A0AS3H6Ny1kBBdI...\n"
          ]
        }
      ],
      "source": [
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "\n",
        "\n",
        "def get_access_token():\n",
        "  \"\"\"\n",
        "  Finds default credentials and returns a valid access token.\n",
        "  Works for both standard users and Workforce Identity users who have already logged in.\n",
        "  \"\"\"\n",
        "  if WORKFORCE_POOL_ID and WORKFORCE_PROVIDER_ID:\n",
        "    if not access_token:\n",
        "      access_token = gcloud_access_token_wif()\n",
        "    return access_token\n",
        "\n",
        "  # Find the application default credentials configured for the environment.\n",
        "  creds, project_id = google.auth.default()\n",
        "  # Create a transport request object to refresh the credentials.\n",
        "  request = google.auth.transport.requests.Request()\n",
        "  # Refresh the credentials to ensure the token is valid and not expired.\n",
        "  creds.refresh(request)\n",
        "  return creds.token\n",
        "\n",
        "# --- Get and print the token ---\n",
        "access_token = get_access_token()\n",
        "print(f\"\\nAccess Token: {access_token[:20]}...\") # Print first 20 chars for security\n",
        "\n",
        "\n",
        "def gcloud_access_token_wif():\n",
        "  ato = !gcloud auth print-access-token\n",
        "  ato = list(ato)[0].strip()\n",
        "  return ato\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kQ2knsnpahLp",
      "metadata": {
        "id": "kQ2knsnpahLp"
      },
      "source": [
        "# 5.0 Import Search Queries, Expected Answers and Golden URLs from File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30170a2e-d956-4d93-bd63-82c8b30e2b67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30170a2e-d956-4d93-bd63-82c8b30e2b67",
        "outputId": "cb263551-cfe1-4e5e-a2ed-de63420a091f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCS path: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/input_queries.csv\n",
            "Reading data from GCS bucket: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/input_queries.csv\n",
            "\n",
            "--- DataFrame Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 190 entries, 0 to 189\n",
            "Data columns (total 3 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   search_query     190 non-null    object\n",
            " 1   expected_answer  190 non-null    object\n",
            " 2   golden_url       190 non-null    object\n",
            "dtypes: object(3)\n",
            "memory usage: 4.6+ KB\n",
            "\n",
            "--- DataFrame Head ---\n",
            "                                        search_query  \\\n",
            "0  Which seeded player from South Korea successfu...   \n",
            "1  In the second qualifier's draw, what was the f...   \n",
            "\n",
            "                                     expected_answer  \\\n",
            "0                Cho Yoon-jeong, who was seeded 5th.   \n",
            "1  Colombia Fabiola Zuluaga defeated Tunisia Seli...   \n",
            "\n",
            "                                          golden_url  \n",
            "0  gs://wikipedia_source_bucket1213092/wikipedia-...  \n",
            "1  gs://wikipedia_source_bucket1213092/wikipedia-...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "file_name = \"confluence_qa_pairs.csv\"\n",
        "# 1. Define the GCS path to your CSV file.\n",
        "gcs_uri = f\"gs://{bucket}/{folder}/{input_file_name}.csv\"\n",
        "print(f\"GCS path: {gcs_uri}\")\n",
        "\n",
        "# 2. IMPORTANT: Define the original column names from your CSV file that you want to read.\n",
        "#    Replace these placeholder names with the actual names in your file.\n",
        "original_cols_to_read = ['search_query', 'expected_answer', 'golden_url']\n",
        "\n",
        "# Define the new names for your DataFrame columns.\n",
        "new_column_names = {\n",
        "    'search_query': 'search_query',\n",
        "    'expected_answer': 'expected_answer',\n",
        "    'golden_url': 'golden_url'\n",
        "}\n",
        "\n",
        "# 3. Read only the specified columns from GCS and rename them in one step.\n",
        "print(f\"Reading data from GCS bucket: {gcs_uri}\")\n",
        "golden_data = pd.read_csv(\n",
        "    gcs_uri,\n",
        "    usecols=original_cols_to_read  # Only load these specific columns\n",
        ").rename(columns=new_column_names) # Rename them to your desired names\n",
        "\n",
        "# 4. Display the first 10 rows and columns to confirm it loaded correctly.\n",
        "print(\"\\n--- DataFrame Info ---\")\n",
        "golden_data.info()\n",
        "\n",
        "print(\"\\n--- DataFrame Head ---\")\n",
        "print(golden_data.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xnGJPrkPa0CD",
      "metadata": {
        "id": "xnGJPrkPa0CD"
      },
      "source": [
        "# 6.0 Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iwJ0z-GkbRyn",
      "metadata": {
        "id": "iwJ0z-GkbRyn"
      },
      "source": [
        "## 6.1 Get Search Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "304142f6-0d36-465a-8f44-40edf3a3fb88",
      "metadata": {
        "id": "304142f6-0d36-465a-8f44-40edf3a3fb88"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import logging\n",
        "import subprocess\n",
        "\n",
        "# Configure a basic logger to see output\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def get_search_results(query, num=K):\n",
        "    try:\n",
        "\n",
        "        access_token = get_access_token()\n",
        "\n",
        "        url = f\"https://{evaluation_region}-discoveryengine.googleapis.com/v1alpha/projects/{project_num}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/{serving_config_id}:search\"\n",
        "\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {access_token}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        data = {\n",
        "            \"servingConfig\": f\"projects/{project_num}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/{serving_config_id}\",\n",
        "            \"query\": query,\n",
        "            \"pageSize\": num,\n",
        "            \"session\":f\"projects/{project_num}/locations/{location}/collections/default_collection/engines/{engine_id}/sessions/-\"\n",
        "        }\n",
        "\n",
        "        search_response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "        if search_response.status_code == 200:\n",
        "            try:\n",
        "                return search_response.json()#['results'][0]['document']['derivedStructData']['link']\n",
        "            except requests.exceptions.JSONDecodeError:\n",
        "                logger.error(f\"Query '{query}' returned status 200 but response was not valid JSON. Body: {search_response.text}\")\n",
        "                return {}\n",
        "        else:\n",
        "            logger.error(\n",
        "                f\"API call for query '{query}' failed with status {search_response.status_code}: {search_response.reason}\\n\"\n",
        "                f\"Response Body: {search_response.text}\"\n",
        "            )\n",
        "            return {}\n",
        "\n",
        "    # THIS IS THE FIX: The 'except' block completes the 'try' statement\n",
        "    except (subprocess.CalledProcessError, requests.exceptions.RequestException, NameError) as e:\n",
        "        logger.error(f\"An error occurred during the search process: {e}\")\n",
        "        # Return an empty dictionary to allow the main loop to continue safely\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jTi7Du6ya-vn",
      "metadata": {
        "id": "jTi7Du6ya-vn"
      },
      "source": [
        "## 6.2 Get Answer Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3045f562-22a1-4a4c-840c-0fe9206a99ef",
      "metadata": {
        "id": "3045f562-22a1-4a4c-840c-0fe9206a99ef"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "# Assume logger and other variables (creds_token, evaluation_region, etc.) are defined\n",
        "\n",
        "def get_answer_results(query, queryId, session_name):\n",
        "    try:\n",
        "        access_token = get_access_token()\n",
        "\n",
        "        url = f\"https://{evaluation_region}-discoveryengine.googleapis.com/v1alpha/projects/{project_num}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/{serving_config_id}:answer\"\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {access_token}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        # The data dictionary with corrected Python syntax\n",
        "        data = {\n",
        "            \"servingConfig\": f\"projects/{project_num}/locations/{location}/collections/default_collection/engines/{engine_id}/servingConfigs/{serving_config_id}\",\n",
        "            \"query\": {\"text\": query, \"queryId\": queryId},\n",
        "            \"session\": session_name,\n",
        "            \"relatedQuestionsSpec\": {\"enable\": True},\n",
        "            \"answerGenerationSpec\": {\n",
        "                \"ignoreAdversarialQuery\": True,\n",
        "                \"ignoreNonAnswerSeekingQuery\": False,\n",
        "                \"ignoreLowRelevantContent\": True,\n",
        "                \"multimodalSpec\": {},\n",
        "                \"includeCitations\": True\n",
        "            }\n",
        "        }\n",
        "\n",
        "        search_response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "        if search_response.status_code == 200:\n",
        "            try:\n",
        "                # Returns the full JSON response if successful\n",
        "                return search_response.json()\n",
        "            except requests.exceptions.JSONDecodeError:\n",
        "                logger.error(f\"Query '{query}' returned status 200 but response was not valid JSON. Body: {search_response.text}\")\n",
        "                return {}\n",
        "        else:\n",
        "            logger.error(\n",
        "                f\"API call for query '{query}' failed with status {search_response.status_code}: {search_response.reason}\\n\"\n",
        "                f\"Response Body: {search_response.text}\"\n",
        "            )\n",
        "            return {}\n",
        "\n",
        "    except (requests.exceptions.RequestException, NameError) as e:\n",
        "        logger.error(f\"An error occurred during the search process: {e}\")\n",
        "        # Return an empty dictionary to allow the main loop to continue safely\n",
        "        return {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yw_QzjrVbEvY",
      "metadata": {
        "id": "yw_QzjrVbEvY"
      },
      "source": [
        "## 6.3 Get Full Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d394de-864b-4bf0-992d-5c1038affd1d",
      "metadata": {
        "id": "63d394de-864b-4bf0-992d-5c1038affd1d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import datetime\n",
        "\n",
        "# Assuming get_search_results() and get_answer_results() are defined correctly\n",
        "\n",
        "def get_full_response_dict(query: str, K: int) -> dict:\n",
        "    \"\"\"\n",
        "    Processes a query to get search and answer results and returns them in a dictionary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- Step 1: Get session info from a search call ---\n",
        "        search_data = get_search_results(query, K)\n",
        "        session_info = search_data.get('sessionInfo', {})\n",
        "        session_name = session_info.get('name')\n",
        "        queryId = session_info.get('queryId')\n",
        "\n",
        "\n",
        "        #print(get_search_results(query, K))\n",
        "\n",
        "        # --- Step 2: Get the generative answer ---\n",
        "        if queryId and session_name:\n",
        "            answer_data = get_answer_results(query, queryId, session_name)\n",
        "            #print (get_answer_results(query, queryId, session_name))\n",
        "        else:\n",
        "            print(\"Warning: Could not find session_name or queryId to get an answer.\")\n",
        "            answer_data = {}\n",
        "\n",
        "        # --- Step 3: Extract all data points and build the dictionary ---\n",
        "        if answer_data:\n",
        "            # Safely get the nested 'answerText'\n",
        "            answer_details = answer_data.get('answer', {})\n",
        "            answer_text = answer_details.get('answerText')\n",
        "\n",
        "            answerQueryToken = answer_data.get('answerQueryToken')\n",
        "            attributionToken = search_data.get('attributionToken')\n",
        "\n",
        "            # Extract all result links\n",
        "            response_urls = []\n",
        "            relevance_scores = []\n",
        "            structured_search_results = []\n",
        "            for result in search_data.get('results', []):\n",
        "                link_struct={}\n",
        "                link = result.get('document', {}).get('derivedStructData', {}).get('link')\n",
        "                relevance_score = result.get('modelScores', {}).get('relevance_score',{}).get('values',[])[0]\n",
        "\n",
        "\n",
        "                if link:\n",
        "                    link_struct = {\n",
        "                        \"link\": link,\n",
        "                        \"relevance_score\": relevance_score\n",
        "                    }\n",
        "                    structured_search_results.append(link_struct)\n",
        "\n",
        "\n",
        "            # Create the final dictionary and return it\n",
        "            response_dict = {\n",
        "                'query': query,\n",
        "                'answer': answer_text,\n",
        "                'attributionToken': attributionToken,\n",
        "                'answerQueryToken': answerQueryToken,\n",
        "                'searchResults': structured_search_results\n",
        "\n",
        "            }\n",
        "            return response_dict\n",
        "\n",
        "        else:\n",
        "            print(\"Failed to get a valid answer from the API.\")\n",
        "            # Return an empty dictionary on failure\n",
        "            return {}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        # Return an empty dictionary on error\n",
        "        return {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FYJR-DZUbZ8J",
      "metadata": {
        "id": "FYJR-DZUbZ8J"
      },
      "source": [
        "# 7.0 Main Processing Loop - Get Answers with Top K Search Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1494bd2c-a95d-4ef6-bf73-124505030a7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1494bd2c-a95d-4ef6-bf73-124505030a7c",
        "outputId": "a3c2dd98-9697-42f9-88c3-881b7d87b743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting new run. Evaluation Run ID: gcs_test_20250813_111354\n",
            "\n",
            "--- Starting Search Query Processing in Batches (GCS) ---\n",
            "Processing 190 rows in 10 batches...\n",
            "\n",
            "--- Processing Batch 1/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  68%|██████▊   | 13/19 [01:43<00:51,  8.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 19/19 [02:30<00:00,  8.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing queries: 100%|██████████| 19/19 [02:39<00:00,  8.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving batch results to: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354/batch_1.csv\n",
            "✅ Batch 1 saved to GCS.\n",
            "\n",
            "--- Processing Batch 2/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 19/19 [02:32<00:00,  8.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving batch results to: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354/batch_2.csv\n",
            "✅ Batch 2 saved to GCS.\n",
            "\n",
            "--- Processing Batch 3/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  26%|██▋       | 5/19 [00:32<01:40,  7.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  89%|████████▉ | 17/19 [02:05<00:15,  7.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 19/19 [02:28<00:00,  7.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving batch results to: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354/batch_3.csv\n",
            "✅ Batch 3 saved to GCS.\n",
            "\n",
            "--- Processing Batch 4/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  58%|█████▊    | 11/19 [01:15<01:01,  7.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 19/19 [02:25<00:00,  7.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving batch results to: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354/batch_4.csv\n",
            "✅ Batch 4 saved to GCS.\n",
            "\n",
            "--- Processing Batch 5/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  37%|███▋      | 7/19 [00:51<01:37,  8.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  63%|██████▎   | 12/19 [01:32<00:57,  8.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  84%|████████▍ | 16/19 [02:03<00:22,  7.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 19/19 [02:31<00:00,  7.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving batch results to: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354/batch_5.csv\n",
            "✅ Batch 5 saved to GCS.\n",
            "\n",
            "--- Processing Batch 6/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  37%|███▋      | 7/19 [00:46<01:28,  7.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  74%|███████▎  | 14/19 [01:37<00:36,  7.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  84%|████████▍ | 16/19 [01:52<00:22,  7.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 19/19 [02:23<00:00,  7.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving batch results to: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354/batch_6.csv\n",
            "✅ Batch 6 saved to GCS.\n",
            "\n",
            "--- Processing Batch 7/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 19/19 [02:26<00:00,  7.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n",
            "Saving batch results to: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354/batch_7.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch 7 saved to GCS.\n",
            "\n",
            "--- Processing Batch 8/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  21%|██        | 4/19 [00:23<01:34,  6.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  32%|███▏      | 6/19 [00:38<01:28,  6.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  63%|██████▎   | 12/19 [01:28<01:00,  8.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing queries:  68%|██████▊   | 13/19 [01:36<00:51,  8.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 19/19 [02:31<00:00,  7.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving batch results to: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354/batch_8.csv\n",
            "✅ Batch 8 saved to GCS.\n",
            "\n",
            "--- Processing Batch 9/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  26%|██▋       | 5/19 [00:30<01:32,  6.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing queries:  32%|███▏      | 6/19 [00:36<01:25,  6.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  79%|███████▉  | 15/19 [01:48<00:32,  8.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 19/19 [02:24<00:00,  7.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n",
            "Saving batch results to: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354/batch_9.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch 9 saved to GCS.\n",
            "\n",
            "--- Processing Batch 10/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  21%|██        | 4/19 [00:21<01:28,  5.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries:  89%|████████▉ | 17/19 [01:56<00:15,  7.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: list index out of range\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing queries: 100%|██████████| 19/19 [02:20<00:00,  7.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving batch results to: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354/batch_10.csv\n",
            "✅ Batch 10 saved to GCS.\n",
            "\n",
            "--- Finalizing Results from GCS ---\n",
            "Merging 10 temporary CSV files from GCS...\n",
            "\n",
            "Saving final merged results to GCS path: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/eval_output_gcs_test_20250813_111354.csv...\n",
            "✅ Successfully saved final results to GCS: gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/eval_output_gcs_test_20250813_111354.csv\n",
            "\n",
            "Cleaning up temporary files from gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/temp_batches_gcs_test_20250813_111354...\n",
            "✅ Successfully deleted 10 temporary files from GCS.\n",
            "\n",
            "--- Cloud-based batch processing complete. ---\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from google.cloud import storage\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "# --- 1. Define a single Evaluation Run ID at the beginning ---\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "assist_run_id = f\"{connector}_{timestamp}\"\n",
        "print(f\"🚀 Starting new run. Search & Assist Run with ID: {assist_run_id}\")\n",
        "\n",
        "# --- Define GCS paths ---\n",
        "TEMP_GCS_FOLDER = f\"{folder}/temp_batches_{assist_run_id}\" # Temp folder is now unique to the run\n",
        "TEMP_GCS_DIR_URI = f\"gs://{bucket}/{TEMP_GCS_FOLDER}\"\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def process_row(row):\n",
        "    \"\"\"\n",
        "    Function to process a single row of the DataFrame.\n",
        "    (This function remains unchanged)\n",
        "    \"\"\"\n",
        "    query = row['search_query']\n",
        "    api_results_dict = get_full_response_dict(query, K)\n",
        "    output_data = row.to_dict()\n",
        "    output_data.update(api_results_dict)\n",
        "    return output_data\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Main Processing Loop\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n--- Starting Search Query Processing in Batches (GCS) ---\")\n",
        "\n",
        "individual_gcs_paths = []\n",
        "tqdm.pandas(desc=\"Processing queries\")\n",
        "\n",
        "# 2. Split the DataFrame into batches\n",
        "list_of_batches = np.array_split(golden_data, len(golden_data) // BATCH_SIZE + 1)\n",
        "total_batches = len(list_of_batches)\n",
        "print(f\"Processing {len(golden_data)} rows in {total_batches} batches...\")\n",
        "\n",
        "# 3. Loop through and process each batch\n",
        "for i, batch_df in enumerate(list_of_batches):\n",
        "    current_batch_num = i + 1\n",
        "    print(f\"\\n--- Processing Batch {current_batch_num}/{total_batches} ---\")\n",
        "\n",
        "    if batch_df.empty:\n",
        "        print(\"Skipping empty batch.\")\n",
        "        continue\n",
        "\n",
        "    # A. Apply the processing function\n",
        "    results_list = batch_df.progress_apply(process_row, axis=1).tolist()\n",
        "    processed_batch_df = pd.DataFrame(results_list)\n",
        "\n",
        "    # B. Add the consistent eval_run_id to the batch DataFrame\n",
        "    processed_batch_df['assist_run_id'] = assist_run_id\n",
        "\n",
        "    # C. Write batch results to a temporary CSV file in GCS\n",
        "    batch_gcs_uri = f\"{TEMP_GCS_DIR_URI}/batch_{current_batch_num}.csv\"\n",
        "    print(f\"Saving batch results to: {batch_gcs_uri}\")\n",
        "    processed_batch_df.to_csv(batch_gcs_uri, index=False)\n",
        "    individual_gcs_paths.append(batch_gcs_uri)\n",
        "    print(f\"✅ Batch {current_batch_num} saved to GCS.\")\n",
        "    #add a delay in order not to hit rate limits\n",
        "    time.sleep(1)\n",
        "\n",
        "# 4. --- Merge, Save Final File, and Clean Up GCS ---\n",
        "print(\"\\n--- Finalizing Results from GCS ---\")\n",
        "\n",
        "if not individual_gcs_paths:\n",
        "    print(\"Warning: No batch files were created. Final DataFrame is empty.\")\n",
        "else:\n",
        "    # A. Merge all individual CSVs directly from GCS\n",
        "    print(f\"Merging {len(individual_gcs_paths)} temporary CSV files from GCS...\")\n",
        "    evaluation_df = pd.concat(\n",
        "        (pd.read_csv(gcs_uri) for gcs_uri in individual_gcs_paths),\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "    # B. Save the final merged DataFrame to its final destination in GCS\n",
        "    # The output filename now includes the run ID for better tracking\n",
        "    final_output_file_name = f\"{output_file_name}_{assist_run_id}.csv\"\n",
        "    final_gcs_uri = f\"gs://{bucket}/{folder}/{final_output_file_name}\"\n",
        "\n",
        "    print(f\"\\nSaving final merged results to GCS path: {final_gcs_uri}...\")\n",
        "    evaluation_df.to_csv(final_gcs_uri, index=False)\n",
        "    print(f\"✅ Successfully saved final results to GCS: {final_gcs_uri}\")\n",
        "\n",
        "    # C. Clean up by deleting the temporary files from GCS\n",
        "    print(f\"\\nCleaning up temporary files from {TEMP_GCS_DIR_URI}...\")\n",
        "    try:\n",
        "        storage_client = storage.Client(project=project)\n",
        "        bucket = storage_client.bucket(bucket)\n",
        "        blobs_to_delete = list(bucket.list_blobs(prefix=TEMP_GCS_FOLDER))\n",
        "\n",
        "        if blobs_to_delete:\n",
        "            bucket.delete_blobs(blobs_to_delete)\n",
        "            print(f\"✅ Successfully deleted {len(blobs_to_delete)} temporary files from GCS.\")\n",
        "        else:\n",
        "            print(\"No temporary files found to delete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during GCS cleanup: {e}\")\n",
        "\n",
        "print(\"\\n--- Cloud-based batch processing complete. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vXS7JhPV1eOF",
      "metadata": {
        "id": "vXS7JhPV1eOF"
      },
      "source": [
        "# 8.0 Answer Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4FdvqhTZb9IR",
      "metadata": {
        "id": "4FdvqhTZb9IR"
      },
      "source": [
        "## 8.1. Defintion of Evaluation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "7f1ddd35-2bfc-4747-be52-41b86d5d0515",
      "metadata": {
        "id": "7f1ddd35-2bfc-4747-be52-41b86d5d0515"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from google import genai\n",
        "import traceback\n",
        "\n",
        "\n",
        "# This is the prompt to validate the quality of the answer results and rate it on a scale\n",
        "EVALUATION_PROMPT = \"\"\"\n",
        "You are a meticulous and objective expert evaluator. Your task is to compare a generated `Answer` to a ground-truth `Expected Answer`, strictly considering the context of the original `Query`. Your evaluation must determine if the `Expected Answer` is factually and semantically contained within the generated `Answer`.\n",
        "\n",
        "You must rate the match on the scale of 0 to 5 defined below. Your response must be ONLY a single, valid JSON object with two keys: \"rating\" (an integer from 0-5) and \"justification\" (a brief string, no more than 1-2 sentences, explaining your rating based on the scale). Do not add any text before or after the JSON object.\n",
        "\n",
        "---\n",
        "[RATING SCALE DEFINITION]\n",
        "\n",
        "- **5 (Excellent Match):** The `Answer` perfectly contains all the information from the `Expected Answer`. It is factually correct, complete, and concise, with no significant missing or extraneous information.\n",
        "\n",
        "- **4 (Very Good Match):** The `Answer` contains all the key information from the `Expected Answer` but may include minor, harmless additional details or have slightly different phrasing that doesn't alter the meaning.\n",
        "\n",
        "- **3 (Good/Partial Match):** The `Answer` contains the main point of the `Expected Answer` but is missing some important secondary details, or it contains a notable amount of extra, less relevant information.\n",
        "\n",
        "- **2 (Fair/Minor Match):** The `Answer` contains some correct elements from the `Expected Answer` but misses the primary point or is factually incomplete in a significant way.\n",
        "\n",
        "- **1 (Poor Match):** The `Answer` touches on the topic of the `Query` but provides information that is mostly irrelevant or fails to align with the `Expected Answer`.\n",
        "\n",
        "- **0 (No Match):** The `Answer` contains no relevant or correct information from the `Expected Answer`. It is completely off-topic, factually wrong, or a refusal to answer.\n",
        "---\n",
        "\n",
        "[TASK]\n",
        "Query: \"{query}\"\n",
        "Expected Answer: \"{expected_answer}\"\n",
        "Answer: \"{answer}\"\n",
        "\n",
        "Your Response:\n",
        "\"\"\"\n",
        "\n",
        "# This is the prompt to validate if the golden link is found in the search link results\n",
        "LINK_PROMPT =\"\"\"\n",
        "Your one and only task is to respond with either TRUE or FALSE, and nothing else.\n",
        "\n",
        "You are a meticulous and objective expert evaluator. Your task is to compare a generated `searchResults` to a ground-truth `goldenLink`. The 'goldenLink' contains a page ID. Try to find if this page ID is present within the search results.\n",
        "\n",
        "The links do not have to match exactly, but the document name must match. Your final response must be the word TRUE or FALSE, and nothing else.\n",
        "\n",
        "\n",
        "---\n",
        "[TASK]\n",
        "goldenLink: \"{goldenLink}\"\n",
        "searchResults: \"{searchResults}\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Initialize the Model\n",
        "client = genai.Client(vertexai=True, project=project, location=vertex_region)\n",
        "MODEL_ID = \"gemini-2.5-flash\"\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_answer(row):\n",
        "    \"\"\"\n",
        "    Calls Gemini to evaluate an answer.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        answer = str(row['answer'])\n",
        "        query = str(row['search_query'])\n",
        "        expected_answer = str(row['expected_answer'])\n",
        "\n",
        "        prompt = EVALUATION_PROMPT.format(\n",
        "            query=query,\n",
        "            expected_answer=expected_answer,\n",
        "            answer=answer\n",
        "        )\n",
        "        # The GenerationConfig class is accessed from `genai` now\n",
        "\n",
        "                # Instead of a genai.GenerationConfig object, use a simple dictionary.\n",
        "        # 1. Use a dictionary for the configuration.\n",
        "        generation_config_dict = {\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"temperature\": 0\n",
        "        }\n",
        "\n",
        "        # 2. Use your original, correct method call, passing the dictionary to the 'config' parameter.\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=prompt,\n",
        "            config=generation_config_dict\n",
        "        )\n",
        "\n",
        "        # This check is crucial to handle blocked responses\n",
        "        if not response.candidates:\n",
        "            justification = f\"Error: Model returned an empty or blocked response. Reason: {response.prompt_feedback}\"\n",
        "            print(justification) # Print error for debugging\n",
        "            return pd.Series({'rating': -1, 'justification': justification})\n",
        "\n",
        "        result_dict = json.loads(response.text)\n",
        "\n",
        "        # This is the successful return path\n",
        "        return pd.Series({\n",
        "            'rating': int(result_dict.get('rating', -1)),\n",
        "            'justification': str(result_dict.get('justification', ''))\n",
        "        })\n",
        "    except Exception as e:\n",
        "        # This general except block catches ALL other errors and ensures a Series is returned\n",
        "        error_message = f'An unexpected error occurred: {e}'\n",
        "        print(error_message) # Print error for debugging\n",
        "        return pd.Series({'rating': -1, 'justification': error_message})\n",
        "\n",
        "\n",
        "def evaluate_link(row):\n",
        "    \"\"\"\n",
        "    Calls Gemini to evaluate links and returns the full error if one occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        goldenLink = str(row['golden_url'])\n",
        "        search_results_list = str(row['searchResults'])\n",
        "\n",
        "        prompt = LINK_PROMPT.format(\n",
        "            goldenLink=goldenLink,\n",
        "            searchResults=search_results_list,\n",
        "        )\n",
        "        generation_config_dict = {\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"temperature\": 0\n",
        "        }\n",
        "\n",
        "        # 2. Use your original, correct method call, passing the dictionary to the 'config' parameter.\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=prompt,\n",
        "            config=generation_config_dict\n",
        "        )\n",
        "\n",
        "        return pd.Series({\n",
        "            'link_evaluation': response.text,\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"--- An error occurred. Capturing full traceback. ---\")\n",
        "        full_error_details = traceback.format_exc()\n",
        "        print(full_error_details)\n",
        "        return pd.Series({\n",
        "            'link_evaluation': None\n",
        "        })\n",
        "\n",
        "\n",
        "def evaluate_row_fully(row):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    A wrapper function that calls both evaluate_answer and evaluate_link\n",
        "    for a single row and combines their results into one Series.\n",
        "    \"\"\"\n",
        "    # Call the first evaluation function\n",
        "    answer_result = evaluate_answer(row)\n",
        "\n",
        "    # Call the second evaluation function\n",
        "    link_result = evaluate_link(row)\n",
        "\n",
        "    # Rename the outputs to prevent column name collisions.\n",
        "    answer_result = answer_result.rename({\n",
        "        'rating': 'answer_rating',\n",
        "        'justification': 'answer_justification'\n",
        "    })\n",
        "\n",
        "    # Combine the results from both functions into a single pandas Series\n",
        "    full_result = pd.concat([answer_result, link_result])\n",
        "\n",
        "    return full_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xKAVLYHG4dZG",
      "metadata": {
        "id": "xKAVLYHG4dZG"
      },
      "source": [
        "## 8.2 Load the Evaluation Data from File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "d37de2af-812f-4140-ab58-f50167c13419",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d37de2af-812f-4140-ab58-f50167c13419",
        "outputId": "625f9d0c-dfa2-4acd-8efb-55a45bb271a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded gs://wikipedia_source_bucket1213092/wikipedia-articles-test-2/eval_output_gcs_test_20250813_111354.csv.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "evaluation_df =[]\n",
        "#Uncomment this line, if a specific file should be used for the evaluation\n",
        "#final_output_file_name = 'eval_output_gcs_test_20250813_111354'\n",
        "\n",
        "\n",
        "# 2. Construct the full GCS URI for the output file.\n",
        "input_file = f\"{final_output_file_name}.csv\"\n",
        "input_gcs_uri = f\"gs://{bucket}/{folder}/{input_file}\"\n",
        "\n",
        "try:\n",
        "    evaluation_df =  pd.read_csv(\n",
        "    input_gcs_uri\n",
        ")\n",
        "    print(f\"Successfully loaded {input_gcs_uri}.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'search_eval.csv' not found. Please ensure the file from the previous step exists.\")\n",
        "    # Create a dummy dataframe for demonstration if file not found\n",
        "    evaluation_df = pd.DataFrame({\n",
        "        'answer': [\"Gemini is a family of multimodal models.\", \"The sky is blue due to Rayleigh scattering.\"],\n",
        "        'expected_answer': [\"Gemini is a family of models.\", \"The sky appears blue because of how the atmosphere interacts with sunlight.\"]\n",
        "    })\n",
        "#print(evaluation_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DeZQUKCu1kD-",
      "metadata": {
        "id": "DeZQUKCu1kD-"
      },
      "source": [
        "## 8.3 Test the evluation with a single row\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "import google.auth\n",
        "\n",
        "from google.colab import userdata # <-- Import userdata to access secrets\n",
        "\n",
        "\n",
        "# --- 2. Prepare the Data for the SDK ---\n",
        "test_data = {\n",
        "    'search_query': [\"what is Gemini?\"],\n",
        "    'expected_answer': [\"Gemini is magic\"],\n",
        "    'answer': [\"Gemini is a AI model made by Google\"],\n",
        "    'golden_url' : [[\"https://somerandomlink.com/type/1234/thegoldendocument.csv\"]],\n",
        "    'searchResults' : [[\n",
        "        {'source': 'gs://wikipedia_source_bucket1213092/wikipedia-articles/2002 US Open  Womens singles qualifying_a96b60.pdf'},\n",
        "        {'source': 'gs://wikipedia_source_bucket1213092/wikipedia-articles/thegoldendocument.csv'},\n",
        "        {'source': 'gs://wikipedia_source_bucket1213092/wikipedia-articles/2022 European Speed Skating Championships  Womens team sprint_abd5c1.pdf'}\n",
        "    ]]\n",
        "}\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "print(\"\\n--- Running evaluation on the single test row ---\")\n",
        "test_row = test_df.iloc[0]\n",
        "\n",
        "# --- 3. Create and Run the Evaluation Task ---\n",
        "print(\"\\n--- Defining and running the evaluation task ---\")\n",
        "\n",
        "\n",
        "evaluation_result = evaluate_row_fully(test_row)\n",
        "\n",
        "# --- 4. Display the Results ---\n",
        "print(\"\\n--- Evaluation Result ---\")\n",
        "# The result object has a convenient method to convert it to a DataFrame\n",
        "\n",
        "print(evaluation_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcs3iRrbd3Hi",
        "outputId": "786b8e0c-9d85-43d2-e9bc-43fe1b179067"
      },
      "id": "Qcs3iRrbd3Hi",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Running evaluation on the single test row ---\n",
            "\n",
            "--- Defining and running the evaluation task ---\n",
            "\n",
            "--- Evaluation Result ---\n",
            "answer_rating                                                           0\n",
            "answer_justification    The generated answer states Gemini is an AI mo...\n",
            "link_evaluation                                                    \"TRUE\"\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YINFyP_1cPfq",
      "metadata": {
        "id": "YINFyP_1cPfq"
      },
      "source": [
        "## 8.4 Run the evaluation and save output to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af369a9-444c-47f9-8c1d-e67ec7bdbe58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9af369a9-444c-47f9-8c1d-e67ec7bdbe58",
        "outputId": "ffa3296c-18c9-46de-ede5-8b1b81ab93e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting AI-based evaluation of answers in batches ---\n",
            "Processing 190 rows in 10 batches of up to 20 rows each.\n",
            "\n",
            "--- Processing Batch 1/10 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating answer quality:  47%|████▋     | 9/19 [01:18<01:30,  9.06s/it]"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "BATCH_SIZE = 20\n",
        "TEMP_BATCH_DIR = \"temp_evaluation_batches\" # Local temporary directory for batch CSVs\n",
        "\n",
        "\n",
        "# define the eval run id\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "eval_run_id = f\"{connector}_{timestamp}\"\n",
        "\n",
        "# --- Main Evaluation Loop ---\n",
        "print(\"\\n--- Starting AI-based evaluation of answers in batches ---\")\n",
        "\n",
        "# Ensure the necessary column exists before proceeding\n",
        "if 'expected_answer' not in evaluation_df.columns:\n",
        "    print(\"Error: 'expected_answer' column not found. Stopping evaluation.\")\n",
        "else:\n",
        "    # 1. Setup for batch processing\n",
        "    os.makedirs(TEMP_BATCH_DIR, exist_ok=True) # Create temp dir for batch files\n",
        "    individual_csv_paths = [] # To store paths of batch CSVs for final merge\n",
        "    tqdm.pandas(desc=\"Evaluating answer quality\")\n",
        "\n",
        "    # Split the DataFrame into batches\n",
        "    list_of_batches = np.array_split(evaluation_df, len(evaluation_df) // BATCH_SIZE + 1)\n",
        "    total_batches = len(list_of_batches)\n",
        "\n",
        "    print(f\"Processing {len(evaluation_df)} rows in {total_batches} batches of up to {BATCH_SIZE} rows each.\")\n",
        "\n",
        "    # 2. Loop through each batch\n",
        "    for i, batch_df in enumerate(list_of_batches):\n",
        "        current_batch_num = i + 1\n",
        "        print(f\"\\n--- Processing Batch {current_batch_num}/{total_batches} ---\")\n",
        "\n",
        "        if batch_df.empty:\n",
        "            print(\"Skipping empty batch.\")\n",
        "            continue\n",
        "\n",
        "        # A. Apply the evaluation function to the current batch\n",
        "        # Using .progress_apply for a progress bar within the batch\n",
        "        batch_results = batch_df.progress_apply(evaluate_row_fully, axis=1)\n",
        "\n",
        "        if isinstance(batch_results, pd.Series):\n",
        "            batch_results = batch_results.to_frame().T\n",
        "\n",
        "        # B. Join results back to the batch DataFrame\n",
        "        # Define columns that might already exist and need to be replaced\n",
        "        cols_to_replace = ['answer_rating', 'answer_justification', 'link_evaluation']\n",
        "        existing_cols_to_drop = [col for col in cols_to_replace if col in batch_df.columns]\n",
        "\n",
        "        if existing_cols_to_drop:\n",
        "            batch_df = batch_df.drop(columns=existing_cols_to_drop)\n",
        "\n",
        "        processed_batch_df = batch_df.join(batch_results)\n",
        "\n",
        "        # C. Add metadata\n",
        "        processed_batch_df['connector'] = connector\n",
        "        processed_batch_df['evaluation_timestamp'] = eval_run_id\n",
        "\n",
        "        # D. Write individual batch results to a local CSV file\n",
        "        batch_file_path = os.path.join(TEMP_BATCH_DIR, f\"batch_{current_batch_num}.csv\")\n",
        "        processed_batch_df.to_csv(batch_file_path, index=False)\n",
        "        individual_csv_paths.append(batch_file_path)\n",
        "        print(f\"✅ Batch {current_batch_num} saved locally to: {batch_file_path}\")\n",
        "\n",
        "        # E. Append results to BigQuery\n",
        "        table_name = f\"{connector}_validation\"\n",
        "        destination_table = f\"{output_dataset}.{table_name}\"\n",
        "\n",
        "        print(f\"Appending batch {current_batch_num} to BigQuery table: {project}.{destination_table}...\")\n",
        "        processed_batch_df.to_gbq(\n",
        "            destination_table=destination_table,\n",
        "            project_id=project,\n",
        "            if_exists='append', # Append results for each batch\n",
        "            progress_bar=True\n",
        "        )\n",
        "        print(f\"✅ Successfully appended batch {current_batch_num} to BigQuery.\")\n",
        "\n",
        "    # 3. --- Post-Loop: Merge individual CSVs and Clean Up ---\n",
        "    print(\"\\n--- Finalizing Results ---\")\n",
        "\n",
        "    # A. Merge all individual CSVs into one DataFrame\n",
        "    if not individual_csv_paths:\n",
        "        print(\"Warning: No batch CSV files were created to merge.\")\n",
        "    else:\n",
        "        print(f\"Merging {len(individual_csv_paths)} individual CSV files...\")\n",
        "        merged_df = pd.concat((pd.read_csv(f) for f in individual_csv_paths), ignore_index=True)\n",
        "\n",
        "        # B. Save the final merged CSV to Google Cloud Storage\n",
        "        output_file = f\"{output_file_name}.csv\"\n",
        "        output_gcs_uri = f\"gs://{bucket}/{folder}/{output_file}\"\n",
        "\n",
        "        print(f\"Saving final merged CSV to GCS path: {output_gcs_uri}...\")\n",
        "        merged_df.to_csv(output_gcs_uri, index=False)\n",
        "        print(f\"✅ Successfully saved merged results to GCS: {output_gcs_uri}\")\n",
        "\n",
        "        # C. Clean up by deleting the local temporary files and directory\n",
        "        print(\"Cleaning up temporary batch files...\")\n",
        "        for file_path in individual_csv_paths:\n",
        "            try:\n",
        "                os.remove(file_path)\n",
        "            except OSError as e:\n",
        "                print(f\"Error deleting file {file_path}: {e}\")\n",
        "        try:\n",
        "            os.rmdir(TEMP_BATCH_DIR)\n",
        "            print(f\"✅ Successfully removed temporary directory: {TEMP_BATCH_DIR}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error removing directory {TEMP_BATCH_DIR}: {e}\")\n",
        "\n",
        "print(\"\\n--- Evaluation process complete. ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "conda-base-py",
      "name": "workbench-notebooks.m131",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel) (Local)",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}